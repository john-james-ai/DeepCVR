{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# REMOVE-CELL\n",
    "import os\n",
    "home = \"/home/john/projects/DeepCVR/\"\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(section_22_data_acquisition)=\n",
    "# Data Acquisition and Ingestion\n",
    "Volume, voracity and variety, three of the n-Vs of Big Data, characterize the information assets that enhance insight, decision making, and process automation. Yet, machine learning algorithms and deep learning models require structured data that is appropriately normalized, standardized and formatted. \n",
    "\n",
    "In this section, we obtain, process, and prepare our data for the analysis, and modeling stages of this project. This section workflow includes: \n",
    "\n",
    "1. **Data Inspection**: Characterize the data preparation effort from representative sample, \n",
    "2. **Data Pipeline Design**: target data model design, framework selection, and data pipeline design, \n",
    "3. **Extract**: Extract full dataset from source,   \n",
    "4. **Transform**: Process and prepare data for analysis and modeling,   \n",
    "5. **Load**: Load the data into database for analysis,\n",
    "6. **Data Inspection**: Inspect data vis-a-vis data model\n",
    "\n",
    "Salient points of interest include:    \n",
    "\n",
    "- Extract Transform Load (ETL) Design Pattern,   \n",
    "- Integrating declarative, functional, and object oriented programming paradigms,\n",
    "- Data pipeline design and implementation,    \n",
    "- Database design and normalization, and  \n",
    "- Parallel Data Processing with Apache Spark, PySpark and Pandas\n",
    "\n",
    "## Data Inspection\n",
    "Our moderately sized source data\n",
    "\n",
    "We begin with a \n",
    "\n",
    "The object of this section is  \n",
    "This section presents the design and implementation of a simple, automated, and reproducible data acquisition and ingestion process  \n",
    "\n",
    "This section presents  the  to  data i and the algorithms  machine learning and deep learning models that   \n",
    "Analysis, inference and predictive analytics require data \n",
    "Wrangling, munging, cleansing, and normalizing data are among the few non-negotiable, truly irreducible components of the machine learning project lifecycle.  Analysis, inference, and predictive analytics require data are sensitive to data quality  determinates of AI value, impact, and return on investment. Bigger, higher-velocity data, disparate data silos, shorter data 'sell-by-dates', and increasingly complex workflows demand thoughtful, principled and disciplined methodologies for discovering, organizing, integrating and preparing data for analytics, and predictive modeling. \n",
    "\n",
    "In this section, we design, build and fly a data acquisition and preparation workflow conceived for data transparency, reproducibility, and reuse. Our territory is the nexus of AI, deep learning innovation, digital marketing, and massive customer behavior data. And our pursuit of second-order, frame-breaking advancements in conversion rate prediction needs data in a useable format and quality. \n",
    "\n",
    "{numref}`ali_ccp_size`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset\n",
    "\n",
    "```{table} Dataset Counts\n",
    ":name: ali_ccp_counts\n",
    "|       Users       |     Items     |   Impressions  |     Clicks    |    Conversions   |\n",
    "|:-----------------:|:-------------:|:--------------:|:-------------:|:----------------:|\n",
    "|          400,000  |    4,300,000  |    84,000,000  |    3,400,000  |          18,000  |\n",
    "```\n",
    "\n",
    "Reviewing the prior section, our dataset \n",
    "\n",
    "## Data Acquisition and Ingestion: Design Principlies\n",
    "As we kick-off this data management effort, reportedly, the most time-intensive phase of the machine learning project lifecycle, let's take a moment to align on a few design principles that will shape the implementation effort. Regarding data migration, five tenets come to mind:\n",
    "\n",
    "1. The power and virtue of **simplicity**.\n",
    "2. The **right practice** is best practice \n",
    "3. **Design** time is non-negotiable.\n",
    "4. **Declarative** and **functional** programming paradigms for data pipelines \n",
    "5. Design databases for **robustness** and **durability**.  \n",
    "\n",
    "As we move through the design and implementation of our workflow, we'll revisit and expand upon theses ideas.\n",
    "\n",
    "## Data Acquisition and Ingestion: Source to Target Model\n",
    "The first step in our workflow design is to briefly describe the source data, present the target database design and to draw a field level mapping from source to the target model.\n",
    "\n",
    "{image}`s2t`: Source to Target Database Model\n",
    "\n",
    "```{figure} ../images/s2t.png\n",
    "---\n",
    "name: s2t\n",
    "align: center\n",
    "alt: Source to Target Model\n",
    "---\n",
    "Source to Target (S2T) Model\n",
    "```\n",
    "### Source Data\n",
    "Recall from the prior introduction section, our source data are split into training and test sets, each approximating 50% of the data. We'll design based upon the training set and apply the same transformations to the test set prior to the modeling stages. As shown in {numref}`s2t` (left-side), the source training set contains two files:\n",
    "\n",
    "- a **sample skeleton** where each row represents a sample or an advertising impression. A sample record contains three sections:\n",
    "  - **id section** comprised of the primary key, sample_id,\n",
    "  - **label section** containing the click and conversion labels,  \n",
    "  - **features section** consisting of a features_list, a number of features in the list, and common_features_index, a foreign key reference to the common_features_train file\n",
    "  \n",
    "- common_features_train represents collections or lists of features that are common among many samples in the sample skeleton file. It contains:\n",
    "  - **id section** containing the primary key, common_features_index\n",
    "  - **features section** This shares a similar structure as the sample skeleton file and includes a features_list and the number of features in the list.\n",
    "  \n",
    "As mentioned, the features_lists in the sample skeleton and common features files share a common format. Concretely, the features_list are lists of feature structures, separated by the ASCII character **0x01**. A feature structure represents a single feature or dimension and its value or metric and contains:   \n",
    "- feature_id,    \n",
    "- feature_name, and\n",
    "- feature_value\n",
    "\n",
    "ASCII strings delimit the elements in the feature structure and are **0x02** and **0x03** respectively.\n",
    "\n",
    "That describes the source training file. Next we layout the target data model.\n",
    "\n",
    "### Target Data Model\n",
    "\n",
    "Our target model will persist as a relational database containing the following tables:\n",
    "\n",
    "- a **sample** table consisting of a sample_id, click and conversion labels, and a common_feature_index, a foreign key reference to the common_feature_group table\n",
    "- a **feature** *reference* table listing each feature in terms of its global id and name,  \n",
    "- a **sample_feature** table comprised of one or more feature structures (id, value) for each sample, and \n",
    "- a **common_feature_group** table holding feature structures commmon among many samples. \n",
    "\n",
    "Next, we characterize the mapping to the target model.\n",
    "\n",
    "### Source to Target Mapping\n",
    "Our database mapping is quite straight forward and can be described in terms of four movements: three from the sample skeleton source file and the remaining movement from the common features file. \n",
    "\n",
    "- **Movement #1**   \n",
    "  Source: sampleskeleton   \n",
    "  Target: sample table     \n",
    "  Fields:\n",
    "    - sample_id\n",
    "    - click_label\n",
    "    - conversion_label\n",
    "    - common_features_index\n",
    "    - target_label (A single multivalue target derived from the two binary targets)\n",
    "\n",
    "- **Movement #2**   \n",
    "  Source: sampleskeleton   \n",
    "  Target: sample_feature    \n",
    "  Fields:   \n",
    "    sample_id   \n",
    "    feature_id   \n",
    "    feature_value   \n",
    "    The above fields to be extracted from the features_list field in the source   \n",
    "\n",
    "- **Movement 3**          \n",
    "  Source: sampleskeleton  \n",
    "  Target: feature \n",
    "  Fields: \n",
    "  - feature_id   \n",
    "  - feature_name   \n",
    "  The above fields to be extracted from the features_list fields in the sampleskeleton and common_features source files.  \n",
    "\n",
    "- **Movement 4**   \n",
    "  Source: common_features_train   \n",
    "  Target: common_features   \n",
    "    common_features_index         \n",
    "    feature_id    \n",
    "    feature_value   \n",
    "    The above fields to be extracted from the features_list field in the source  \n",
    "\n",
    "Extracting the features from the feature structures may require a transformation step potentially involving non-vectorized, computationally expensive, row-wise data manipulation. \n",
    "\n",
    "We've specified the source and target data models as well as a field-level mapping. Next, we consider the available programming languages, frameworks and toolsets that will advance our goals of reproducibility, reuse, and data transparency.\n",
    "\n",
    "## Data Acquisition and Ingestion: Programming Environment and Framework\n",
    "At this stage, we consider the programming environment and data processing frameworks that will connect our data management imperatives with our design principles. \n",
    "\n",
    "But for two factors, this data migration could be reasonably classified as a trivial undertaking. First, our moderately sized data source of about 85 million observations will manifest a database in the billions of rows. In modern contexts, it would be a stretch to label this as a big data project, yet the workload for the prevalent open-source and commercial databases is non-trivial. Scalability is a factor. Second unpacking the feature data may present computational inefficiencies best remedied by a map reduce or split, apply combine concurrency pattern.\n",
    "\n",
    "Keeping it simple, a lean Python-based environment running a MySQL backend end addresses our scalability imperative. Pandas provided in-memory analytics, augmented by Apache Spark's Pandas User Defined Function facility will allow us to partition and distribute DataFrame processing across multiple CPU cores on a standalone machine or clusters of nodes. Wrapping this up into a docker container (which will be done as soon as this writer learns docker) will advance reproducibility and portability. \n",
    "\n",
    "With that, the software environment and framework selection efforts are complete. Next, we design the custom components of the workflow, transitioning us from design to implementation.\n",
    "\n",
    "## Data Acquisition and Ingestion: Workflow Design\n",
    "Here, we put forward the design for the custom components of an automated, reproducible data migration pipeline to support analytics, model design, development and evaluation. The design is simple, disquietly so, it expresses right practice,  for this \n",
    "Designing the data migration worfklow, data pipeline, or directed acyclic graph (DAG) in detail crystallized certain design principles, and advanced thinking in unexpected ways. \n",
    "\n",
    " thinking on others with unexpected consequences. In this secti  others and  design principles and extended others. certain design principles forced some    crystalized and extended thinking  \n",
    "will be defined in **declarative** YAML-based configuration files.  Whereas imperative programming approaches decompose problems into a collection of computational steps that must be carried out, declarative programming abstracts away control flow for logic required to perform an action, by instead stating the task or desired outcome without explicitly listing the commands required to complete the task. Defining pipelines in this way emphasizes 'what' must be done rather than 'how'. Achieving simplicity through restraint, declarative pipelines have a restricted, simpler syntax, allowing for less error-prone, more structured, manageable, and scalable pipeline development. Easier to read, write, and maintain, declarative pipelines are the central idea behind modern 'Pipeline as Code' approaches.\n",
    "\n",
    "For discrete tasks, operators should emphasize a stateless **functional** programmining paradigm that defines operations only in terms of functions or methods, their inputs, and their return values. Small functions are deterministic and composable into modules that cannot be affected by any mutable state or unintended side-effect. Defining operations in a functional paradigm supports modularity, easier debugging, testing, and verification.   \n",
    "\n",
    "### Leverage Right Technologies\n",
    "Since the centralized repositories of the 1970's, the **extract transform load** design pattern has been doctrine for acquiring, ingesting, and integrating data from disparate external sources into a functional data store for business and predictive analytics. \n",
    "\n",
    "**Python**, our lingua franca supports functional, object oriented, procedural and scripting programming paradigms for rapid development and prototyping. It cleans up after itself. Memory for Python objects is dynamically allocated on a private heap and garbage collected by the Python Memory Manager automatically when no longer needed. \n",
    "\n",
    "For in-memory analytics, **Pandas** further enhances Python's memory cost performance through efficient storage of numeric and categorical data types. The Pandas DataFrame API has become somewhat of a standard, implemented and extended by an ecosystem that includes Koalas, a DataFrame interface on top of **Apache Spark**. In fact, we will be exploiting Spark's Pandas User-Defined Function (UDF) facility with **PySpark** to parallelize and scale DataFrame processing across multiple CPU cores.\n",
    "\n",
    "Finally, our database, consisting of millions of advertising impressions and billions of user and item feature structures will be running MySQL. Version 8.0.28 includes a host of improvements designed to take full advantage of modern hardware and operating system resources and efficiencies.\n",
    "\n",
    "### Embrace Parallelism\n",
    "Time-to-value is minimized by performing as many parallel ETL integrations as one's infrastructure allows. With Apache Spark split-apply-combine pattern, we can partition DataFrame processing and dispatch partitions for processing over multiple CPU cores on a single host or a cluster of nodes. Python's Threading package provides a Python-native concurrency option as well.\n",
    "\n",
    "### Design Database for Analytics\n",
    "Design target databases for flexible query handling and real-time in-database analytics. For the analysis start with third normal form (3NF), a relational database schema design originally defined by E.F. Codd in the early 1970's. A 3NF database minimizes data anomalies, guarantees referential integrity and ensures that every non-key attribute provides true evidence about the table key, the whole key, and nothing but the key, \"so help me Codd\" {cite}diehrDatabaseManagement1989`.  \n",
    "\n",
    "### Exploit Automation\n",
    "Our final design principle connects reproducibility with a productivity motive. Automation minimizes manual data manipulation errors, and reduces the need for costly human operators.  \n",
    "\n",
    "## Source to Target Model\n",
    "We've articulated a set of guiding design principles, and made some architectural choices. Let us formalize the target database design and a mapping from the source data schema to our target data model.\n",
    "\n",
    "## Extract Transform Load Package\n",
    "\n",
    "The class model for the extract-transform-load (ETL) pipeline are put forward in {numref}`etl_dag`\n",
    "\n",
    "```{figure} ../images/etl.png\n",
    "---\n",
    "name: etl\n",
    "align: center\n",
    "alt: Extract Transform Load\n",
    "---\n",
    "Extract Transform Load\n",
    "```\n",
    "This diagram depicts four categories of objects that collectively represent our ETL implementation.  \n",
    "\n",
    "- **Declarative Pipeline Definitions**: The three documents on the left in vertical orientation contain the dag or pipeline commands in declarative language.   \n",
    "- **DagBuilder and Dag Classes**: Moving to the right, the DagBuilder takes a dag definition as input and constructs an extract, transform, or load DAG.\n",
    "- **Operator and Subclasses**: The Operator abstract class and twelve Operator subclasses perform task level implementation logic\n",
    "- **CSV Files and Database**: The CSV files contain the source data and the target database is running within a MySQL instance.\n",
    "\n",
    "To initiate the overall ETL, the DagBuilder is instantiated with one of the three DAG definition documents. Each DAG document specifies one or more task declarations. A task declaration consists of:\n",
    "\n",
    "- a task identifier, \n",
    "- the name of Operator class that implements the task,  \n",
    "- the module containing the Operator class,\n",
    "- the parameters with which the Operator is to be instantiated.\n",
    "\n",
    "The build method iteratively instantiates an associated Operator object with the parameters specified in the task declaration and appends the Operator object to the DAG object. Once the DAG document has been processed, the build method returns the DAG executable, and the DAG construction is complete. The execute method on the DAG is called, which, in turn invokes the execute method on the next Operator object in the DAGs list of Operators. \n",
    "\n",
    "Let's see the ETL in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from deepcvr.base.dag import DagBuilder, Dag\n",
    "from deepcvr.utils.config import config_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "The extract stage is comprised of two tasks. The first task is executed by the S3Downloader executable in the deepcvr.data.extract directory. It downloads the data from the production folder of the deepcvr-data AWS bucket and stores the data in the data/external directory. The second task, is performed by the Decompress class in the deepcvr.data.extract module. It unpacks the data from its GZIP archive and stores it in csv format in the data/raw/ directory. The yaml configuration file 'extract.yml' file below defines this short DAG."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "dag_id: extract_dag\n",
    "dag_description: Extract phase of the ETL\n",
    "tasks:\n",
    "  1:\n",
    "    task: S3Downloader\n",
    "    module: deepcvr.data.extract\n",
    "    task_id: 1\n",
    "    task_name: s3_download\n",
    "    task_params:\n",
    "      bucket: deepcvr-data\n",
    "      folder: production/\n",
    "      destination: data/external/\n",
    "      force: False\n",
    "  2:\n",
    "    task: Decompress\n",
    "    module: deepcvr.data.extract\n",
    "    task_id: 2\n",
    "    task_name: decompress_source_files\n",
    "    task_params:\n",
    "      source: data/external/\n",
    "      destination: data/raw/\n",
    "      force: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet loads the extract DAG definition above, constructs the DAG, invokes the run method on the DAG, and the extract stage completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: extract\t\tTask 1:\ts3_download_test\tComplete.\tDuration:0.37 seconds.\n",
      "Module: extract\t\tTask 2:\tdecompress_source_files\tComplete.\tDuration:0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "config_filepath = \"tests/test_config/extract.yaml\"\n",
    "config = config_dag(config_filepath)\n",
    "dag = DagBuilder(config=config).build()\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/development/staged/sample_skeleton_train.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
    "\n",
    "Our aim for the transform step, is a fully realized 3rd normal form target data model free of without redundancy, logical inconsistencies, transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
    "\n",
    "Notwithstanding, transforming our features from a series of strings to rows of feature structures will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, they are dispatched to a configurable number of CPU cores running in parallel, then the results are combined into a final single DataFrame. \n",
    "\n",
    "The source code for the transform step \n",
    "Viola! \n",
    "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
    "\n",
    "Analyzing our data\n",
    "\n",
    "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
    "\n",
    "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
    "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
    "\n",
    "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
    "\n",
    " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
    "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
    "\n",
    "![ERD](/jbook/images/ETL-DAG.png)\n",
    "\n",
    "\n",
    " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
    " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
    " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
    "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
    "\n",
    "\n",
    "## Extract\n",
    "\n",
    "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
    " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
    "\n",
    "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
    "\n",
    "- quickly illuminate structural or data quality issues \n",
    "- assess the complexity of the integration effort, and\n",
    "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
    "\n",
    "[erd](jbook/images/ERD.png)\n",
    "\n",
    "\n",
    "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
    "\n",
    "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
    "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
    "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
    "\n",
    "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
    "\n",
    "```{table} Sampling Strata\n",
    ":name: sampling_strata\n",
    "\n",
    "| Stratum | Click | Conversion | Proportion | Response                     |\n",
    "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
    "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
    "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
    "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
    "```\n",
    "A sample size \n",
    "\n",
    "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
    "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
    "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
    "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
    "\n",
    "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
    "\n",
    "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
    "\n",
    "\n",
    "\n",
    " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
    "\n",
    "### Core Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Acquisition\n",
    "Wrangling, munging, cleansing and manipulating data are irreducible variables in the machine learning and big data value equation. Statistical inference, predictive analytics, and problem solving with machines and math require data, in the right format, volume, and veracity. In this section, we design, build and execute a simple, automated and reproducible data ingestion pipeline that extracts the data from its source, transforms it into a usable and reliable resource, then loads the data into a database for downstream analysis and modeling. The main components are put forward as follows:\n",
    "\n",
    "## Extract\n",
    "Our ETL pipeline is defined using declarative pipeline syntax - basic statements and expressions which sequence the parameterized tasks that collectively execute the ETL process. First, the data are downloaded from an Amazon S3 instance, unzipped, persisted, and this raw data are registered as assets in the metadata database. Column headings are added, partitions are assigned, andd the data are stored in a staging area for the transformation step. \n",
    "\n",
    "## Transform\n",
    "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
    "\n",
    "Our aim for the transform step, is a fully realized 3rd normal target data model free of redundancy, and logical inconsistencies, inappropriate and transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
    "\n",
    "Notwithstanding, transforming our feature data will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, and dispatched to one of a configurable number of CPU cores, then results are combined into a final single DataFrame. \n",
    "\n",
    "The source code for the transform step \n",
    "Viola! \n",
    "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
    "\n",
    "Analyzing our data\n",
    "\n",
    "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
    "\n",
    "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
    "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
    "\n",
    "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
    "\n",
    " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
    "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
    "\n",
    "![ERD](/jbook/images/ETL-DAG.png)\n",
    "\n",
    "\n",
    " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
    " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
    " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
    "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
    "\n",
    "\n",
    "## Extract\n",
    "\n",
    "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
    " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
    "\n",
    "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
    "\n",
    "- quickly illuminate structural or data quality issues \n",
    "- assess the complexity of the integration effort, and\n",
    "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
    "\n",
    "[erd](jbook/images/ERD.png)\n",
    "\n",
    "\n",
    "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
    "\n",
    "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
    "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
    "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
    "\n",
    "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
    "\n",
    "```{table} Sampling Strata\n",
    ":name: sampling_strata\n",
    "\n",
    "| Stratum | Click | Conversion | Proportion | Response                     |\n",
    "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
    "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
    "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
    "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
    "```\n",
    "A sample size \n",
    "\n",
    "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
    "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
    "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
    "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
    "\n",
    "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
    "\n",
    "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
    "\n",
    "\n",
    "\n",
    " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
    "\n",
    "### Core Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions = \"data/archive/production/raw/sample_skeleton_train.csv\"\n",
    "df = pd.read_csv(impressions, header=None, index_col=None)\n",
    "df.loc[(df[1]==0) & (df[2]==0)].shape[0] / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "Downloading the data from our S3 instance will take approximately 15 minutes on a standard 40 Mbps internet line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s S3Downloader deepcvr/data/download.py\n",
    "class S3Downloader:\n",
    "    \"\"\"Download operator for Amazon S3 Resources\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket\n",
    "        destination (str): Director to which all resources are to be downloaded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket: str, destination: str, force: bool = False) -> None:\n",
    "        self._bucket = bucket\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "        config = S3Config()\n",
    "        self._s3 = boto3.client(\n",
    "            \"s3\", aws_access_key_id=config.key, aws_secret_access_key=config.secret\n",
    "        )\n",
    "        self._progressbar = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "\n",
    "        object_keys = self._list_bucket_contents()\n",
    "\n",
    "        for object_key in object_keys:\n",
    "            destination = os.path.join(self._destination, object_key)\n",
    "            if not os.path.exists(destination) or self._force:\n",
    "                self._download(object_key, destination)\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Bucket resource {} already exists and was not downloaded.\".format(destination)\n",
    "                )\n",
    "\n",
    "    def _list_bucket_contents(self) -> list:\n",
    "        \"\"\"Returns a list of objects in the designated bucket\"\"\"\n",
    "        objects = []\n",
    "        s3 = boto3.resource(\"s3\")\n",
    "        bucket = s3.Bucket(self._bucket)\n",
    "        for object in bucket.objects.all():\n",
    "            objects.append(object.key)\n",
    "        return objects\n",
    "\n",
    "    def _download(self, object_key: str, destination: str) -> None:\n",
    "        \"\"\"Downloads object designated by the object ke if not exists or force is True\"\"\"\n",
    "\n",
    "        response = self._s3.head_object(Bucket=self._bucket, Key=object_key)\n",
    "        size = response[\"ContentLength\"]\n",
    "\n",
    "        self._progressbar = progressbar.progressbar.ProgressBar(maxval=size)\n",
    "        self._progressbar.start()\n",
    "\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        try:\n",
    "            self._s3.download_file(\n",
    "                self._bucket, object_key, destination, Callback=self._download_callback\n",
    "            )\n",
    "            logger.info(\"Download of {} Complete!\".format(object_key))\n",
    "        except NoCredentialsError:\n",
    "            msg = \"Credentials not available for {} bucket\".format(self._bucket)\n",
    "            raise NoCredentialsError(msg)\n",
    "\n",
    "    def _download_callback(self, size):\n",
    "        self._progressbar.update(self._progressbar.currval + size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = S3Downloader(bucket=S3_BUCKET, destination=DIRECTORY_EXTERNAL)\n",
    "downloader.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Raw Data\n",
    "Here, we extract the compressed files into a raw data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s Extractor deepcvr/data/extract.py\n",
    "class Extractor:\n",
    "    \"\"\"Decompresses a gzip archive, stores the raw data\n",
    "\n",
    "    Args:\n",
    "        source (str): The filepath to the source file to be decompressed\n",
    "        destination (str): The destination directory into which data shall be stored.\n",
    "        filetype (str): The file extension for the uncompressed data\n",
    "        force (bool): Forces extraction even when files already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source: str, destination: str, force: bool = False) -> None:\n",
    "\n",
    "        self._source = source\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        \"\"\"Extracts and stores the data, then pushes filepaths to xCom.\"\"\"\n",
    "        logger.debug(\"\\tSource: {}\\tDestination: {}\".format(self._source, self._destination))\n",
    "\n",
    "        # If all 4 raw files exist, it is assumed that the data have been downloaded\n",
    "        n_files = len(os.listdir(self._destination))\n",
    "        if n_files < 4:\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as tempdir:\n",
    "                # Recursively extract data and store in destination directory\n",
    "                self._extract(source=self._source, destination=tempdir)\n",
    "\n",
    "    def _extract(self, source: str, destination: str) -> None:\n",
    "        \"\"\"Extracts the data and returns the extracted filepaths\"\"\"\n",
    "\n",
    "        logger.debug(\"\\t\\tOpening {}\".format(source))\n",
    "        data = tarfile.open(source)\n",
    "\n",
    "        for member in data.getmembers():\n",
    "            if self._is_csvfile(filename=member.name):\n",
    "                if self._not_exists_or_force(member_name=member.name):\n",
    "                    logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, self._destination))\n",
    "                    data.extract(member, self._destination)  # Extract to destination\n",
    "                else:\n",
    "                    pass  # Do nothing if the csv file already exists and Force is False\n",
    "\n",
    "            else:\n",
    "                logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, destination))\n",
    "                data.extract(member, destination)  # Extract to tempdirectory\n",
    "\n",
    "    def _not_exists_or_force(self, member_name: str) -> bool:\n",
    "        \"\"\"Returns true if the file doesn't exist or force is True.\"\"\"\n",
    "        filepath = os.path.join(self._destination, member_name)\n",
    "        return not os.path.exists(filepath) or self._force\n",
    "\n",
    "    def _is_csvfile(self, filename: str) -> bool:\n",
    "        \"\"\"Returns True if filename is a csv file, returns False otherwise.\"\"\"\n",
    "        return \".csv\" in filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor(source=FILEPATH_EXTERNAL_TRAIN, destination=DIRECTORY_RAW)\n",
    "filenames = extractor.execute()\n",
    "os.listdir(DIRECTORY_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Dataset Preprocessing\n",
    "Let's take a preliminary look at the core training dataset.\n",
    "### Core Raw Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TEST_CORE, header=None, index_col=[0], nrows=10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TRAIN_COMMON, header=None, index_col=0, nrows=100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# REMOVE-CELL\n",
    "# References and Notes\n",
    "Refer to  https://www.netquest.com/blog/en/random-sampling-stratified-sampling for sampling techniques"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c1728eb1d2e5aa0ad9cb608f2ae480dc35c5197350e729ffcd56015e38fc7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deepcvr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
