{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# REMOVE-CELL\n",
                "import os\n",
                "home = \"/home/john/projects/DeepCVR/\"\n",
                "os.chdir(home)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(section_22_data_acquisition)=\n",
                "# Data Acquisition\n",
                "Wrangling, munging, cleansing and manipulating data are irreducible variables in the machine learning and big data value equation. Statistical inference, predictive analytics, and problem solving with machines and math require data, in the right format, volume, and veracity. In this section, we design, build and execute a simple, automated and reproducible data ingestion pipeline that extracts the data from its source, transforms it into a usable and reliable resource, then loads the data into a database for downstream analysis and modeling. The main components are put forward as follows:\n",
                "\n",
                "![ETL](jbook/images/ETL-DAG.png)\n",
                "\n",
                "## Extract\n",
                "A DAG object containing the extract, transform, and load objects is instantiated with a pipeline specification - a configuration file containing basic declarative statements, expressions, and instructions for each of the pipeline tasks. The data is downloaded, decompressed, and stored as raw data. Some minor preprocessing is performed, the data is registered in the metadata database and persisted in a staging area pending the next step in the ETL.\n",
                "\n",
                "## Transform\n",
                "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from deepcvr.data.etl import ETL\n",
                "task_id = 0\n",
                "task_name = 'extract_transform_load'\n",
                "params = {\n",
                "    'dataset': 'train',\n",
                "    'mode': 'development'\n",
                "}\n",
                "etl = ETL(task_id=task_id, task_name=task_name, params=params)\n",
                "etl.execute()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPORTS\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"data/development/staged/sample_skeleton_train.csv\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our aim for the transform step, is a fully realized 3rd normal form target data model free of without redundancy, logical inconsistencies, transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
                "\n",
                "Notwithstanding, transforming our features from a series of strings to rows of feature structures will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, they are dispatched to a configurable number of CPU cores running in parallel, then the results are combined into a final single DataFrame. \n",
                "\n",
                "The source code for the transform step \n",
                "Viola! \n",
                "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
                "\n",
                "Analyzing our data\n",
                "\n",
                "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
                "\n",
                "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
                "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
                "\n",
                "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
                "\n",
                " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
                "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
                "\n",
                "![ERD](/jbook/images/ETL-DAG.png)\n",
                "\n",
                "\n",
                " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
                " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
                " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
                "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
                "\n",
                "\n",
                "## Extract\n",
                "\n",
                "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
                " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
                "\n",
                "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
                "\n",
                "- quickly illuminate structural or data quality issues \n",
                "- assess the complexity of the integration effort, and\n",
                "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
                "\n",
                "[erd](jbook/images/ERD.png)\n",
                "\n",
                "\n",
                "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
                "\n",
                "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
                "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
                "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
                "\n",
                "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
                "\n",
                "```{table} Sampling Strata\n",
                ":name: sampling_strata\n",
                "\n",
                "| Stratum | Click | Conversion | Proportion | Response                     |\n",
                "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
                "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
                "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
                "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
                "```\n",
                "A sample size \n",
                "\n",
                "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
                "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
                "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
                "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
                "\n",
                "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
                "\n",
                "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
                "\n",
                "\n",
                "\n",
                " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
                "\n",
                "### Core Data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(section_22_data_acquisition)=\n",
                "# Data Acquisition\n",
                "Wrangling, munging, cleansing and manipulating data are irreducible variables in the machine learning and big data value equation. Statistical inference, predictive analytics, and problem solving with machines and math require data, in the right format, volume, and veracity. In this section, we design, build and execute a simple, automated and reproducible data ingestion pipeline that extracts the data from its source, transforms it into a usable and reliable resource, then loads the data into a database for downstream analysis and modeling. The main components are put forward as follows:\n",
                "\n",
                "## Extract\n",
                "Our ETL pipeline is defined using declarative pipeline syntax - basic statements and expressions which sequence the parameterized tasks that collectively execute the ETL process. First, the data are downloaded from an Amazon S3 instance, unzipped, persisted, and this raw data are registered as assets in the metadata database. Column headings are added, partitions are assigned, andd the data are stored in a staging area for the transformation step. \n",
                "\n",
                "## Transform\n",
                "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
                "\n",
                "Our aim for the transform step, is a fully realized 3rd normal target data model free of redundancy, and logical inconsistencies, inappropriate and transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
                "\n",
                "Notwithstanding, transforming our feature data will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, and dispatched to one of a configurable number of CPU cores, then results are combined into a final single DataFrame. \n",
                "\n",
                "The source code for the transform step \n",
                "Viola! \n",
                "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
                "\n",
                "Analyzing our data\n",
                "\n",
                "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
                "\n",
                "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
                "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
                "\n",
                "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
                "\n",
                " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
                "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
                "\n",
                "![ERD](/jbook/images/ETL-DAG.png)\n",
                "\n",
                "\n",
                " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
                " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
                " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
                "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
                "\n",
                "\n",
                "## Extract\n",
                "\n",
                "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
                " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
                "\n",
                "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
                "\n",
                "- quickly illuminate structural or data quality issues \n",
                "- assess the complexity of the integration effort, and\n",
                "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
                "\n",
                "[erd](jbook/images/ERD.png)\n",
                "\n",
                "\n",
                "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
                "\n",
                "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
                "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
                "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
                "\n",
                "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
                "\n",
                "```{table} Sampling Strata\n",
                ":name: sampling_strata\n",
                "\n",
                "| Stratum | Click | Conversion | Proportion | Response                     |\n",
                "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
                "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
                "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
                "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
                "```\n",
                "A sample size \n",
                "\n",
                "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
                "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
                "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
                "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
                "\n",
                "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
                "\n",
                "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
                "\n",
                "\n",
                "\n",
                " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
                "\n",
                "### Core Data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(section_22_data_acquisition)=\n",
                "# Data Acquisition\n",
                "Wrangling, munging, cleansing and manipulating data are irreducible variables in the machine learning and big data value equation. Statistical inference, predictive analytics, and problem solving with machines and math require data, in the right format, volume, and veracity. In this section, we design, build and execute a simple, automated and reproducible data ingestion pipeline that extracts the data from its source, transforms it into a usable and reliable resource, then loads the data into a database for downstream analysis and modeling. The main components are put forward as follows:\n",
                "\n",
                "## Extract\n",
                "Our ETL pipeline is defined using declarative pipeline syntax - basic statements and expressions which sequence the parameterized tasks that collectively execute the ETL process. First, the data are downloaded from an Amazon S3 instance, unzipped, persisted, and this raw data are registered as assets in the metadata database. Column headings are added, partitions are assigned, andd the data are stored in a staging area for the transformation step. \n",
                "\n",
                "## Transform\n",
                "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
                "\n",
                "Our aim for the transform step, is a fully realized 3rd normal target data model free of redundancy, and logical inconsistencies, inappropriate and transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
                "\n",
                "Notwithstanding, transforming our feature data will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, and dispatched to one of a configurable number of CPU cores, then results are combined into a final single DataFrame. \n",
                "\n",
                "The source code for the transform step \n",
                "Viola! \n",
                "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
                "\n",
                "Analyzing our data\n",
                "\n",
                "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
                "\n",
                "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
                "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
                "\n",
                "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
                "\n",
                " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
                "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
                "\n",
                "![ERD](/jbook/images/ETL-DAG.png)\n",
                "\n",
                "\n",
                " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
                " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
                " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
                "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
                "\n",
                "\n",
                "## Extract\n",
                "\n",
                "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
                " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
                "\n",
                "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
                "\n",
                "- quickly illuminate structural or data quality issues \n",
                "- assess the complexity of the integration effort, and\n",
                "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
                "\n",
                "[erd](jbook/images/ERD.png)\n",
                "\n",
                "\n",
                "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
                "\n",
                "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
                "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
                "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
                "\n",
                "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
                "\n",
                "```{table} Sampling Strata\n",
                ":name: sampling_strata\n",
                "\n",
                "| Stratum | Click | Conversion | Proportion | Response                     |\n",
                "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
                "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
                "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
                "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
                "```\n",
                "A sample size \n",
                "\n",
                "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
                "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
                "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
                "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
                "\n",
                "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
                "\n",
                "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
                "\n",
                "\n",
                "\n",
                " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
                "\n",
                "### Core Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPORTS\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "impressions = \"data/archive/production/raw/sample_skeleton_train.csv\"\n",
                "df = pd.read_csv(impressions, header=None, index_col=None)\n",
                "df.loc[(df[1]==0) & (df[2]==0)].shape[0] / df.shape[0] * 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(section_22_data_acquisition)=\n",
                "# Data Acquisition\n",
                "One of the most challenging problems to solve in deep learning has little to do with neural network architectures, algorithm design, or AI framework selection. Teaching machines to learn without explicitly programming them to do so rests on our ability to acquire, prepare, and serve the right data, of the right quality, quantity and format. In this section, we design, build and execute a simple, automated and reproducible data extraction, transformation and loading (ETL) pipeline.\n",
                "\n",
                "## Data Profile\n",
                "Our dataset consists of a training and test set, each comprised of an impressions file, and a common features file as indicated below.\n",
                "\n",
                "| Name            | Set      | Filename                  | Size (GB) |\n",
                "|-----------------|----------|---------------------------|-----------|\n",
                "| Core     | Training | sample_skeleton_train.csv | 10        |\n",
                "| Common Features | Training | common_features_train.csv | 8         |\n",
                "| Core     | Test     | sample_skeleton_test.csv  | 10        |\n",
                "| Common Features | Test     | common_features_test.csv  | 10        |\n",
                "\n",
                "Let's take a look.\n",
                "\n",
                "### Core Data\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "This proj\n",
                "Our goal is to extract our data from its source, transform it into a structure and format consistent with our target data model, then load the data into the target database for downstream processing, cleaning, selection, analytics and modeling. This extract-transform-load (ETL) design pattern is the  remains the  the centralized data repositories of the 1970's, through the emergence of the data warehouses in the 1990s, the extract-transform-load (ETL) has emerged as datesOur ex  then load the data into our target database for downstream processing, analytics and modeling. . data selection,  into the target dataa structure and format appropriate for analysis and modeling, then load the data into our target database. \n",
                " mafor anlysis  a structure and format that supports analysis and   source data from  \n",
                "We begin our design process with a detailed, field-level analysis and mapping of our source data to our target data model. One of the most important data integration tasks, source-to-target mapping (STTM) exercise:  \n",
                "- ensures that the source data conforms with the target data model,    \n",
                "- confirms that the source data meets the structural, integrity, and data quality requirements of the analytics and modeling efforts, and    \n",
                "- provides a sense of the complexity of the integration effort, and\n",
                "- reveals the logic required to convert the source data to the target data model.\n",
                "\n",
                "### Source-to-Target Mapping (STTM)\n",
                "```{figure} ../images/STTM.png\n",
                "---\n",
                "height: 500px\n",
                "width: 900px\n",
                "name: sttm\n",
                "---\n",
                "Source to Target Model\n",
                "```\n",
                "A mini-mapping analysi {ref}`sttm` depicts,  \n",
                "Our S2M As indicated in {ref}`sttm`, our source data are comprised of two types of datasets: a CoreData and a CommonFeatures da containing id, label, and feature data for each impression, and a common features dataset which aggregates features common to many impressions. The target data model contains the following four tables.\n",
                "\n",
                "- **Core**: id, and label data for each impression,    \n",
                "- **CoreFeatures**: The feature data for each immpression, represented in 3rd normal formFeature data in 3rd normal form \n",
                "\n",
                "- Our target data model contains \n",
                "- Core table\n",
                "\n",
                " the following four tables:    \n",
                "- \n",
                " contain  and  feature, and a foreign key reference to a seco the sample id\n",
                "\n",
                ", problems, risks,     our target environment, and  system,    examing our source dataa source to target (S2T)The first step in our design process is develop a field-level mapping from the data sources to the target system. This Source To Target (S2T) analysis: \n",
                "- ensures that the source data exists and meets requirements for analysis and model development,  \n",
                "- illuminates risks,  complexities, risks,   for  by analysit exiists   reveals data conversion logic, business rules, loading frequency, business logic that  informs a host of design decisions, from loading frequency  includes not only a field level mapping from  serves  a blueprint for our data acquisition solution   characterize the strutusource data and target data structure \n",
                "Recall from the prior section that our source data are comprised of two related datasets: a core dataset containing the labels and features for each impression, and a common features dataset which aggregates features common to many impressions. The goal of the data acquisition pipeline is to convert the source data depicted on the left of {ref}`sttm` to the structure and format represented on the right side of {ref}`sttm`. To accomplish this, we will \n",
                "The goal of the data acquisition pipeline is to:\n",
                "1. **Extract** the CoreData and CommonFeaturesData datasets depicted on the left of {ref}`sttm` to a local staging area, \n",
                "2. **Transform** the CoreData and CommonFeaturesData datasets into the four target environment datasets described on right of {ref}`sttm`, and finally, \n",
                "3. **Load** the four datasets into the target relational database management system.\n",
                "\n",
                "\n",
                "```{figure} ../images/STTM.png\n",
                "---\n",
                "height: 500px\n",
                "width: 900px\n",
                "name: sttm\n",
                "---\n",
                "Source to Target Model\n",
                "```\n",
                "Let's review the mapping of the CoreData and CoreFeatures.\n",
                "#### CoreData Dataset Mapping\n",
                "The CoreData datasets map to an Core table, and a CoreFeatures table. The Core table has all the fields contained in the CoreData datasets with one exception: the feature list. Feature lists found in the CoreData and CommonFeaturesData datasets are variable length lists of feature structures, each containing a feature name, feature id, and feature value. The lists of feature structures will be parsed, normalized, and stored in a separate CoreFeatures table where each observation corresponds to a single feature for an impression. \n",
                "\n",
                "#### CommonFeaturesData Dataset Mapping\n",
                "Similarly, the CommonFeaturesData dataset is comprised of rows of common feature lists observed across many impressions. This dataset will map to a CommonFeaturesSummary and a CommonFeatures table.  The CommonFeaturesSummary simply stores the common_feature_index and the number of feature structures in feature lists stored in the CommonFeatures table.  \n",
                "\n",
                "### Directed Acyclic Graph \n",
                "We've described the ETL process as a pipeline through which data flows sequentially from one end to the other. In practice, the metaphor is a bit misleading. Data isn't literally flowing from one end of a single tube to the other. Rather, ETL processes may be complex, non-linear, networks, of objects, tasks performed on those objects, and dependencies between tasks. A more apt theoretical framework for reasoning about ETL workflows can be borrowed from graph theory.\n",
                "\n",
                "A graph is a pair $G=(V,E)$, where: \n",
                "- $V$ is a set of vertices, and \n",
                "- $E$ is a set of paired vertices or edges.\n",
                "\n",
                "In a *directed* graph or *digraph*, each edge $E \\subseteq \\{(x,y)|(x,y)\\in V^2$ and $x\\ne y\\}$ between a pair of vertices has a polarity or orientation from one vertex to another. For instance, the pair of vertices may be tasks to perform within a data pipeline, and the edge between them may represent the constraint that the end task must initiate after the start task has completed.  \n",
                "\n",
                "A *path* graph is a sequence of edges in a graph in which the ending vertex or task of each edge in a sequence is the same as the starting vertex or task of the next edge in the sequence. More formally, a graph of order $n\\ge2$ is a graph in which the vertices can be listed in an order $\\{v_1,v_2,\\dots,v_n\\}$ such that the edges are $\\{v_i,v_{i+1}\\}$ for  $i=1,2,\\dots,n-1$. When the starting vertex of the path is the same as the ending vertex of the path, a cycle has been formed. \n",
                "\n",
                "Finally, a directed *acyclic* graph has at least one topological ordering of its vertices into a sequence, such that the start vertex of every directed edge occurs earlier in the sequence than the ending vertex of that edge. Further, any graph that has topological ordering cannot have any cycles because the edge into the earliest vertex of the cycle would have to be oriented in the wrong direction. \n",
                "\n",
                "Hence, graph theory provides a powerfully, simple and mathematically elegant language for  for expressing    \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "Given their mathematical properties, DAGs have been used in a wide range of scientific, computational, biological, and sociological applications. \n",
                "\n",
                "Now, we can represent our ETL process as a directed acyclic graph $G=(V,E)$ where $V$ is a set of objects or vertices, and $E$ is the set edges or tasks directionally connecting objects. The high-level ETL DAG is summarized in {ref}`etl_dag`.\n",
                "\n",
                "```{figure} ../images/ETLDAG.png\n",
                "---\n",
                "height: 500px\n",
                "width: 900px\n",
                "name: etl_dag\n",
                "---\n",
                "Extract Transform Load DAG\n",
                "```\n",
                "In the \n",
                "## Extract "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# Imports\n",
                "# External Modules\n",
                "import os\n",
                "import boto3\n",
                "from botocore.exceptions import NoCredentialsError\n",
                "import logging\n",
                "import progressbar\n",
                "import tarfile\n",
                "import tempfile\n",
                "import numpy as np\n",
                "import numexpr as ne\n",
                "os.environ['NUMEXPR_MAX_THREADS'] = '24'\n",
                "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
                "import pandas as pd\n",
                "pd.set_option('display.max_rows', 500)\n",
                "pd.set_option('display.max_columns', 500)\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "pd.set_option('display.width', 1000)\n",
                "# Logging Configuration\n",
                "# ------------------------------------------------------------------------------------------------ #\n",
                "logging.basicConfig(level=logging.DEBUG)\n",
                "logger = logging.getLogger(__name__)\n",
                "# ------------------------------------------------------------------------------------------------ #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-cell",
                    "remove-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# REMOVE-CELL\n",
                "# Must reset current directory to the project home before importing internal modules\n",
                "home = \"/home/john/projects/DeepCVR/\"\n",
                "os.chdir(home)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# Imports\n",
                "# Internal Modules\n",
                "from deepcvr.utils.config import S3Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-input"
                ]
            },
            "outputs": [],
            "source": [
                "# Constants \n",
                "S3_BUCKET = 'deepcvr-data'\n",
                "DIRECTORY_EXTERNAL = \"data/external\"\n",
                "DIRECTORY_RAW = 'data/raw'\n",
                "DIRECTORY_STAGED = 'data/staged'\n",
                "DIRECTORY_SAMPLE = 'data/sample'\n",
                "FILEPATH_EXTERNAL_TRAIN = os.path.join(DIRECTORY_EXTERNAL, 'taobao_train.tar.gz')\n",
                "FILEPATH_EXTERNAL_TEST = os.path.join(DIRECTORY_EXTERNAL, 'taobao_test.tar.gz')\n",
                "FILEPATH_RAW_TRAIN_CORE = os.path.join(DIRECTORY_RAW,\"sample_skeleton_train.csv\")\n",
                "FILEPATH_RAW_TRAIN_COMMON = os.path.join(DIRECTORY_RAW,\"common_features_train.csv\")\n",
                "FILEPATH_RAW_TEST_CORE = os.path.join(DIRECTORY_RAW,\"sample_skeleton_test.csv\")\n",
                "FILEPATH_RAW_TEST_COMMON = os.path.join(DIRECTORY_RAW,\"common_features_test.csv\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download Data\n",
                "Downloading the data from our S3 instance will take approximately 15 minutes on a standard 40 Mbps internet line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# %load -s S3Downloader deepcvr/data/download.py\n",
                "class S3Downloader:\n",
                "    \"\"\"Download operator for Amazon S3 Resources\n",
                "\n",
                "    Args:\n",
                "        bucket (str): The name of the S3 bucket\n",
                "        destination (str): Director to which all resources are to be downloaded\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, bucket: str, destination: str, force: bool = False) -> None:\n",
                "        self._bucket = bucket\n",
                "        self._destination = destination\n",
                "        self._force = force\n",
                "        config = S3Config()\n",
                "        self._s3 = boto3.client(\n",
                "            \"s3\", aws_access_key_id=config.key, aws_secret_access_key=config.secret\n",
                "        )\n",
                "        self._progressbar = None\n",
                "\n",
                "    def execute(self) -> None:\n",
                "\n",
                "        object_keys = self._list_bucket_contents()\n",
                "\n",
                "        for object_key in object_keys:\n",
                "            destination = os.path.join(self._destination, object_key)\n",
                "            if not os.path.exists(destination) or self._force:\n",
                "                self._download(object_key, destination)\n",
                "            else:\n",
                "                logger.info(\n",
                "                    \"Bucket resource {} already exists and was not downloaded.\".format(destination)\n",
                "                )\n",
                "\n",
                "    def _list_bucket_contents(self) -> list:\n",
                "        \"\"\"Returns a list of objects in the designated bucket\"\"\"\n",
                "        objects = []\n",
                "        s3 = boto3.resource(\"s3\")\n",
                "        bucket = s3.Bucket(self._bucket)\n",
                "        for object in bucket.objects.all():\n",
                "            objects.append(object.key)\n",
                "        return objects\n",
                "\n",
                "    def _download(self, object_key: str, destination: str) -> None:\n",
                "        \"\"\"Downloads object designated by the object ke if not exists or force is True\"\"\"\n",
                "\n",
                "        response = self._s3.head_object(Bucket=self._bucket, Key=object_key)\n",
                "        size = response[\"ContentLength\"]\n",
                "\n",
                "        self._progressbar = progressbar.progressbar.ProgressBar(maxval=size)\n",
                "        self._progressbar.start()\n",
                "\n",
                "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
                "        try:\n",
                "            self._s3.download_file(\n",
                "                self._bucket, object_key, destination, Callback=self._download_callback\n",
                "            )\n",
                "            logger.info(\"Download of {} Complete!\".format(object_key))\n",
                "        except NoCredentialsError:\n",
                "            msg = \"Credentials not available for {} bucket\".format(self._bucket)\n",
                "            raise NoCredentialsError(msg)\n",
                "\n",
                "    def _download_callback(self, size):\n",
                "        self._progressbar.update(self._progressbar.currval + size)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "downloader = S3Downloader(bucket=S3_BUCKET, destination=DIRECTORY_EXTERNAL)\n",
                "downloader.execute()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract Raw Data\n",
                "Here, we extract the compressed files into a raw data directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# %load -s Extractor deepcvr/data/extract.py\n",
                "class Extractor:\n",
                "    \"\"\"Decompresses a gzip archive, stores the raw data\n",
                "\n",
                "    Args:\n",
                "        source (str): The filepath to the source file to be decompressed\n",
                "        destination (str): The destination directory into which data shall be stored.\n",
                "        filetype (str): The file extension for the uncompressed data\n",
                "        force (bool): Forces extraction even when files already exist.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, source: str, destination: str, force: bool = False) -> None:\n",
                "\n",
                "        self._source = source\n",
                "        self._destination = destination\n",
                "        self._force = force\n",
                "\n",
                "    def execute(self) -> None:\n",
                "        \"\"\"Extracts and stores the data, then pushes filepaths to xCom.\"\"\"\n",
                "        logger.debug(\"\\tSource: {}\\tDestination: {}\".format(self._source, self._destination))\n",
                "\n",
                "        # If all 4 raw files exist, it is assumed that the data have been downloaded\n",
                "        n_files = len(os.listdir(self._destination))\n",
                "        if n_files < 4:\n",
                "\n",
                "            with tempfile.TemporaryDirectory() as tempdir:\n",
                "                # Recursively extract data and store in destination directory\n",
                "                self._extract(source=self._source, destination=tempdir)\n",
                "\n",
                "    def _extract(self, source: str, destination: str) -> None:\n",
                "        \"\"\"Extracts the data and returns the extracted filepaths\"\"\"\n",
                "\n",
                "        logger.debug(\"\\t\\tOpening {}\".format(source))\n",
                "        data = tarfile.open(source)\n",
                "\n",
                "        for member in data.getmembers():\n",
                "            if self._is_csvfile(filename=member.name):\n",
                "                if self._not_exists_or_force(member_name=member.name):\n",
                "                    logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, self._destination))\n",
                "                    data.extract(member, self._destination)  # Extract to destination\n",
                "                else:\n",
                "                    pass  # Do nothing if the csv file already exists and Force is False\n",
                "\n",
                "            else:\n",
                "                logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, destination))\n",
                "                data.extract(member, destination)  # Extract to tempdirectory\n",
                "\n",
                "    def _not_exists_or_force(self, member_name: str) -> bool:\n",
                "        \"\"\"Returns true if the file doesn't exist or force is True.\"\"\"\n",
                "        filepath = os.path.join(self._destination, member_name)\n",
                "        return not os.path.exists(filepath) or self._force\n",
                "\n",
                "    def _is_csvfile(self, filename: str) -> bool:\n",
                "        \"\"\"Returns True if filename is a csv file, returns False otherwise.\"\"\"\n",
                "        return \".csv\" in filename\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "extractor = Extractor(source=FILEPATH_EXTERNAL_TRAIN, destination=DIRECTORY_RAW)\n",
                "filenames = extractor.execute()\n",
                "os.listdir(DIRECTORY_RAW)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Core Dataset Preprocessing\n",
                "Let's take a preliminary look at the core training dataset.\n",
                "### Core Raw Training Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(FILEPATH_RAW_TEST_CORE, header=None, index_col=[0], nrows=10000)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we have: \n",
                "\n",
                "| Column | Field                                  |\n",
                "|--------|----------------------------------------|\n",
                "| 0      | Sample-id                              |\n",
                "| 1      | Click Label                            |\n",
                "| 2      | Conversion Label                       |\n",
                "| 3      | Common Features Foreign Key            |\n",
                "| 4      | Number of features in the feature list |\n",
                "| 5      | Feature List                           |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(FILEPATH_RAW_TRAIN_COMMON, header=None, index_col=0, nrows=100)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we have: \n",
                "\n",
                "| Column | Field                                  |\n",
                "|--------|----------------------------------------|\n",
                "| 0      | Sample-id                              |\n",
                "| 1      | Click Label                            |\n",
                "| 2      | Conversion Label                       |\n",
                "| 3      | Common Features Foreign Key            |\n",
                "| 4      | Number of features in the feature list |\n",
                "| 5      | Feature List                           |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "source": [
                "# REMOVE-CELL\n",
                "# References and Notes\n",
                "Refer to  https://www.netquest.com/blog/en/random-sampling-stratified-sampling for sampling techniques"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "b4c1728eb1d2e5aa0ad9cb608f2ae480dc35c5197350e729ffcd56015e38fc7c"
        },
        "kernelspec": {
            "display_name": "Python 3.8.12 ('deepcvr')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
