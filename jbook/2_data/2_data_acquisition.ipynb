{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(section_22_data_acquisition)=\n",
    "# Data Acquisition\n",
    "One of the most challenging problems to solve in deep learning has nothing to do with advanced neural network architectures, algorithm design, hardware configuration or AI framework selection. Solving problems by teaching machines to learn without being explicitly programmed rests on our ability to acquire, prepare, and serve the right data, of the right quality, quantity and format. From data ingestion, through analytics, machine learning and deep learning model training, data pipelines are the fundamental building blocks of artificial intelligence. In this section, we design, build and execute a simple, automated, and reproducible data acquisition and ingestion pipeline.\n",
    "\n",
    "\n",
    " Recall from the prior section, several aspects of our data \n",
    "\n",
    "## Data Acquisition ETL Design\n",
    "### Source to Target Model (STTM)\n",
    "Recall, our source data are comprised of CoreData and CommonFeaturesData datasets. The goal of the data acquisition pipeline is to:\n",
    "1. **Extract** the CoreData and CommonFeaturesData datasets depicted on the left of {ref}`sttm` to a local staging area, \n",
    "2. **Transform** the CoreData and CommonFeaturesData datasets into the four target environment datasets described on right of {ref}`sttm`, then\n",
    "3. **Load** the four datasets into the target relational database management system.\n",
    "\n",
    "The following characterizes the mapping from the source data to the target data environment.\n",
    "\n",
    "```{figure} ../images/STTM.png\n",
    "---\n",
    "height: 500px\n",
    "width: 900px\n",
    "name: sttm\n",
    "---\n",
    "Source to Target Model\n",
    "```\n",
    "Let's review the mapping of the CoreData and CoreFeatures.\n",
    "#### CoreData Dataset Mapping\n",
    "The CoreData datasets map to an Impressions table, and a CoreFeatures table. The Impressions table has all the fields contained in the CoreData datasets with one exception: the feature list. Feature lists found in the CoreData and CommonFeaturesData datasets are variable length lists of feature structures, each containing a feature name, feature id, and feature value. The lists of feature structures will be parsed, normalized, and stored in a separate CoreFeatures table where each observation corresponds to a single feature for an impression. \n",
    "\n",
    "#### CommonFeaturesData Dataset Mapping\n",
    "Similarly, the CommonFeaturesData dataset is comprised of rows of common feature lists observed across many impressions. This dataset will map to a CommonFeaturesSummary and a CommonFeatures table.  The CommonFeaturesSummary simply stores the common_feature_index and the number of feature structures in feature lists stored in the CommonFeatures table.  \n",
    "\n",
    "### Directed Acyclic Graph \n",
    "We've described the ETL process as a pipeline through which data flows sequentially from one end to the other. In practice, the metaphor is a bit misleading. Data isn't literally flowing from one end of a single tube to the other. Rather, ETL processes may be complex, non-linear, networks, of objects, tasks performed on those objects, and dependencies between tasks. A more apt theoretical framework for reasoning about ETL workflows can be borrowed from graph theory.\n",
    "\n",
    "A graph is a pair $G=(V,E)$, where: \n",
    "- $V$ is a set of vertices, and \n",
    "- $E$ is a set of paired vertices or edges.\n",
    "\n",
    "In a *directed* graph or *digraph*, each edge $E \\subseteq \\{(x,y)|(x,y)\\in V^2$ and $x\\ne y\\}$ between a pair of vertices has a polarity or orientation from one vertex to another. For instance, the pair of vertices may be tasks to perform within a data pipeline, and the edge between them may represent the constraint that the end task must initiate after the start task has completed.  \n",
    "\n",
    "A *path* graph is a sequence of edges in a graph in which the ending vertex or task of each edge in a sequence is the same as the starting vertex or task of the next edge in the sequence. More formally, a graph of order $n\\ge2$ is a graph in which the vertices can be listed in an order $\\{v_1,v_2,\\dots,v_n\\}$ such that the edges are $\\{v_i,v_{i+1}\\}$ for  $i=1,2,\\dots,n-1$. When the starting vertex of the path is the same as the ending vertex of the path, a cycle has been formed. \n",
    "\n",
    "Finally, a directed *acyclic* graph has at least one topological ordering of its vertices into a sequence, such that the start vertex of every directed edge occurs earlier in the sequence than the ending vertex of that edge. Further, any graph that has topological ordering cannot have any cycles because the edge into the earliest vertex of the cycle would have to be oriented in the wrong direction. \n",
    "\n",
    "Given their mathematical properties, DAGs have been used in a wide range of scientific, computational, biological, and sociological applications. \n",
    "\n",
    "Now, we can design our ETL process as a directed acyclic graph $G=(V,E)$ where $V$ is a set of objects or vertices, and $E$ is the set edges or tasks directionally connecting objects. The high-level ETL DAG is summarized in {ref}`etl_dag`.\n",
    "\n",
    "```{figure} ../images/ETLDAG.png\n",
    "---\n",
    "height: 500px\n",
    "width: 900px\n",
    "name: etl_dag\n",
    "---\n",
    "Extract Transform Load DAG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# External Modules\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import logging\n",
    "import progressbar\n",
    "import tarfile\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '24'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "# Logging Configuration\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "# ------------------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# REMOVE-CELL\n",
    "# Must reset current directory to the project home before importing internal modules\n",
    "home = \"/home/john/projects/DeepCVR/\"\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Internal Modules\n",
    "from deepcvr.utils.config import S3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Constants \n",
    "S3_BUCKET = 'deepcvr-data'\n",
    "DIRECTORY_EXTERNAL = \"data/external\"\n",
    "DIRECTORY_RAW = 'data/raw'\n",
    "DIRECTORY_STAGED = 'data/staged'\n",
    "DIRECTORY_SAMPLE = 'data/sample'\n",
    "FILEPATH_EXTERNAL_TRAIN = os.path.join(DIRECTORY_EXTERNAL, 'taobao_train.tar.gz')\n",
    "FILEPATH_EXTERNAL_TEST = os.path.join(DIRECTORY_EXTERNAL, 'taobao_test.tar.gz')\n",
    "FILEPATH_RAW_TRAIN_CORE = os.path.join(DIRECTORY_RAW,\"sample_skeleton_train.csv\")\n",
    "FILEPATH_RAW_TRAIN_COMMON = os.path.join(DIRECTORY_RAW,\"common_features_train.csv\")\n",
    "FILEPATH_RAW_TEST_CORE = os.path.join(DIRECTORY_RAW,\"sample_skeleton_test.csv\")\n",
    "FILEPATH_RAW_TEST_COMMON = os.path.join(DIRECTORY_RAW,\"common_features_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "Downloading the data from our S3 instance will take approximately 15 minutes on a standard 40 Mbps internet line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s S3Downloader deepcvr/data/download.py\n",
    "class S3Downloader:\n",
    "    \"\"\"Download operator for Amazon S3 Resources\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket\n",
    "        destination (str): Director to which all resources are to be downloaded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket: str, destination: str, force: bool = False) -> None:\n",
    "        self._bucket = bucket\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "        config = S3Config()\n",
    "        self._s3 = boto3.client(\n",
    "            \"s3\", aws_access_key_id=config.key, aws_secret_access_key=config.secret\n",
    "        )\n",
    "        self._progressbar = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "\n",
    "        object_keys = self._list_bucket_contents()\n",
    "\n",
    "        for object_key in object_keys:\n",
    "            destination = os.path.join(self._destination, object_key)\n",
    "            if not os.path.exists(destination) or self._force:\n",
    "                self._download(object_key, destination)\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Bucket resource {} already exists and was not downloaded.\".format(destination)\n",
    "                )\n",
    "\n",
    "    def _list_bucket_contents(self) -> list:\n",
    "        \"\"\"Returns a list of objects in the designated bucket\"\"\"\n",
    "        objects = []\n",
    "        s3 = boto3.resource(\"s3\")\n",
    "        bucket = s3.Bucket(self._bucket)\n",
    "        for object in bucket.objects.all():\n",
    "            objects.append(object.key)\n",
    "        return objects\n",
    "\n",
    "    def _download(self, object_key: str, destination: str) -> None:\n",
    "        \"\"\"Downloads object designated by the object ke if not exists or force is True\"\"\"\n",
    "\n",
    "        response = self._s3.head_object(Bucket=self._bucket, Key=object_key)\n",
    "        size = response[\"ContentLength\"]\n",
    "\n",
    "        self._progressbar = progressbar.progressbar.ProgressBar(maxval=size)\n",
    "        self._progressbar.start()\n",
    "\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        try:\n",
    "            self._s3.download_file(\n",
    "                self._bucket, object_key, destination, Callback=self._download_callback\n",
    "            )\n",
    "            logger.info(\"Download of {} Complete!\".format(object_key))\n",
    "        except NoCredentialsError:\n",
    "            msg = \"Credentials not available for {} bucket\".format(self._bucket)\n",
    "            raise NoCredentialsError(msg)\n",
    "\n",
    "    def _download_callback(self, size):\n",
    "        self._progressbar.update(self._progressbar.currval + size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n",
      "INFO:__main__:Bucket resource data/external/taobao_test.tar.gz already exists and was not downloaded.\n",
      "INFO:__main__:Bucket resource data/external/taobao_train.tar.gz already exists and was not downloaded.\n"
     ]
    }
   ],
   "source": [
    "downloader = S3Downloader(bucket=S3_BUCKET, destination=DIRECTORY_EXTERNAL)\n",
    "downloader.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Raw Data\n",
    "Here, we extract the compressed files into a raw data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s Extractor deepcvr/data/extract.py\n",
    "class Extractor:\n",
    "    \"\"\"Decompresses a gzip archive, stores the raw data\n",
    "\n",
    "    Args:\n",
    "        source (str): The filepath to the source file to be decompressed\n",
    "        destination (str): The destination directory into which data shall be stored.\n",
    "        filetype (str): The file extension for the uncompressed data\n",
    "        force (bool): Forces extraction even when files already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source: str, destination: str, force: bool = False) -> None:\n",
    "\n",
    "        self._source = source\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        \"\"\"Extracts and stores the data, then pushes filepaths to xCom.\"\"\"\n",
    "        logger.debug(\"\\tSource: {}\\tDestination: {}\".format(self._source, self._destination))\n",
    "\n",
    "        # If all 4 raw files exist, it is assumed that the data have been downloaded\n",
    "        n_files = len(os.listdir(self._destination))\n",
    "        if n_files < 4:\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as tempdir:\n",
    "                # Recursively extract data and store in destination directory\n",
    "                self._extract(source=self._source, destination=tempdir)\n",
    "\n",
    "    def _extract(self, source: str, destination: str) -> None:\n",
    "        \"\"\"Extracts the data and returns the extracted filepaths\"\"\"\n",
    "\n",
    "        logger.debug(\"\\t\\tOpening {}\".format(source))\n",
    "        data = tarfile.open(source)\n",
    "\n",
    "        for member in data.getmembers():\n",
    "            if self._is_csvfile(filename=member.name):\n",
    "                if self._not_exists_or_force(member_name=member.name):\n",
    "                    logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, self._destination))\n",
    "                    data.extract(member, self._destination)  # Extract to destination\n",
    "                else:\n",
    "                    pass  # Do nothing if the csv file already exists and Force is False\n",
    "\n",
    "            else:\n",
    "                logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, destination))\n",
    "                data.extract(member, destination)  # Extract to tempdirectory\n",
    "\n",
    "    def _not_exists_or_force(self, member_name: str) -> bool:\n",
    "        \"\"\"Returns true if the file doesn't exist or force is True.\"\"\"\n",
    "        filepath = os.path.join(self._destination, member_name)\n",
    "        return not os.path.exists(filepath) or self._force\n",
    "\n",
    "    def _is_csvfile(self, filename: str) -> bool:\n",
    "        \"\"\"Returns True if filename is a csv file, returns False otherwise.\"\"\"\n",
    "        return \".csv\" in filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_skeleton_train.csv',\n",
       " 'sample_skeleton_test.csv',\n",
       " 'common_features_test.csv',\n",
       " 'common_features_train.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = Extractor(source=FILEPATH_EXTERNAL_TRAIN, destination=DIRECTORY_RAW)\n",
    "filenames = extractor.execute()\n",
    "os.listdir(DIRECTORY_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Dataset Preprocessing\n",
    "Let's take a preliminary look at the core training dataset.\n",
    "### Core Raw Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23bd0f75de327c60</td>\n",
       "      <td>14</td>\n",
       "      <td>216\u00029181078\u00031.0\u0001301\u00029351665\u00031.0\u0001205\u00025587143\u00031.0\u0001206\u00028315277\u00031.0\u0001207\u00028801026\u00031.0\u0001702\u00029878755\u00032.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23bd0f75de327c60</td>\n",
       "      <td>15</td>\n",
       "      <td>205\u00025662732\u00031.0\u0001206\u00028316893\u00031.0\u0001207\u00028987328\u00031.0\u0001853\u000210020538\u00032.539\u0001702\u00029896652\u00033.58352\u0001508\u000293551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23bd0f75de327c60</td>\n",
       "      <td>12</td>\n",
       "      <td>206\u00028315405\u00031.0\u0001205\u00026539512\u00031.0\u0001301\u00029351665\u00031.0\u0001216\u00029273427\u00031.0\u0001210\u00029100479\u00031.0\u0001210\u00029084127\u00031.0\u0001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23bd0f75de327c60</td>\n",
       "      <td>11</td>\n",
       "      <td>509\u00029686171\u00032.99573\u0001210\u00029068992\u00031.0\u0001207\u00028801026\u00031.0\u0001206\u00028315276\u00031.0\u0001205\u00028010649\u00031.0\u0001210\u00029104804\u0003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>543b0cd53c7d5858</td>\n",
       "      <td>11</td>\n",
       "      <td>206\u00028317093\u00031.0\u0001508\u00029355323\u00032.63906\u0001210\u00029020410\u00031.0\u0001210\u00029045228\u00031.0\u0001210\u00029089073\u00031.0\u0001210\u00029035934\u0003...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2                 3   4                                                                                                    5\n",
       "0                                                                                                                                 \n",
       "1  0  0  23bd0f75de327c60  14  216\u00029181078\u00031.0\u0001301\u00029351665\u00031.0\u0001205\u00025587143\u00031.0\u0001206\u00028315277\u00031.0\u0001207\u00028801026\u00031.0\u0001702\u00029878755\u00032.07...\n",
       "2  0  0  23bd0f75de327c60  15  205\u00025662732\u00031.0\u0001206\u00028316893\u00031.0\u0001207\u00028987328\u00031.0\u0001853\u000210020538\u00032.539\u0001702\u00029896652\u00033.58352\u0001508\u000293551...\n",
       "3  0  0  23bd0f75de327c60  12  206\u00028315405\u00031.0\u0001205\u00026539512\u00031.0\u0001301\u00029351665\u00031.0\u0001216\u00029273427\u00031.0\u0001210\u00029100479\u00031.0\u0001210\u00029084127\u00031.0\u0001...\n",
       "4  0  0  23bd0f75de327c60  11  509\u00029686171\u00032.99573\u0001210\u00029068992\u00031.0\u0001207\u00028801026\u00031.0\u0001206\u00028315276\u00031.0\u0001205\u00028010649\u00031.0\u0001210\u00029104804\u0003...\n",
       "5  0  0  543b0cd53c7d5858  11  206\u00028317093\u00031.0\u0001508\u00029355323\u00032.63906\u0001210\u00029020410\u00031.0\u0001210\u00029045228\u00031.0\u0001210\u00029089073\u00031.0\u0001210\u00029035934\u0003..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TEST_CORE, header=None, index_col=[0], nrows=10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84dceed2e3a667f8</th>\n",
       "      <td>343</td>\n",
       "      <td>101\u000231319\u00031.0\u0001125\u00023438774\u00031.0\u0001126\u00023438779\u00031.0\u0001127\u00023438782\u00031.0\u0001128\u00023864885\u00031.0\u0001129\u00023864888\u00031.0\u000115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000350f0c2121e7</th>\n",
       "      <td>811</td>\n",
       "      <td>127_14\u00023716224\u00031.94591\u0001127_14\u00023514627\u00030.69315\u0001127_14\u00023772871\u00030.69315\u0001127_14\u00023543283\u00031.60944\u0001127_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000091a89d1867ab</th>\n",
       "      <td>7</td>\n",
       "      <td>125\u00023438773\u00031.0\u0001124\u00023438769\u00031.0\u0001122\u00023438761\u00031.0\u0001121\u00023438658\u00031.0\u0001129\u00023864889\u00031.0\u0001128\u00023864885\u00031.0\u0001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001a4114b0ae8bf</th>\n",
       "      <td>231</td>\n",
       "      <td>150_14\u00023916684\u00032.3979\u0001150_14\u00023940798\u00031.07056\u0001150_14\u00023892368\u00031.6259\u0001150_14\u00023914634\u00030.55962\u0001150_14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001def19d7cb335</th>\n",
       "      <td>964</td>\n",
       "      <td>150_14\u00023909150\u00030.84715\u0001150_14\u00023933013\u00034.44265\u0001150_14\u00023934083\u00033.3322\u0001150_14\u00023874258\u00034.09988\u0001150_1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1                                                                                                    2\n",
       "0                                                                                                                         \n",
       "84dceed2e3a667f8  343  101\u000231319\u00031.0\u0001125\u00023438774\u00031.0\u0001126\u00023438779\u00031.0\u0001127\u00023438782\u00031.0\u0001128\u00023864885\u00031.0\u0001129\u00023864888\u00031.0\u000115...\n",
       "0000350f0c2121e7  811  127_14\u00023716224\u00031.94591\u0001127_14\u00023514627\u00030.69315\u0001127_14\u00023772871\u00030.69315\u0001127_14\u00023543283\u00031.60944\u0001127_...\n",
       "000091a89d1867ab    7  125\u00023438773\u00031.0\u0001124\u00023438769\u00031.0\u0001122\u00023438761\u00031.0\u0001121\u00023438658\u00031.0\u0001129\u00023864889\u00031.0\u0001128\u00023864885\u00031.0\u0001...\n",
       "0001a4114b0ae8bf  231  150_14\u00023916684\u00032.3979\u0001150_14\u00023940798\u00031.07056\u0001150_14\u00023892368\u00031.6259\u0001150_14\u00023914634\u00030.55962\u0001150_14...\n",
       "0001def19d7cb335  964  150_14\u00023909150\u00030.84715\u0001150_14\u00023933013\u00034.44265\u0001150_14\u00023934083\u00033.3322\u0001150_14\u00023874258\u00034.09988\u0001150_1..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TRAIN_COMMON, header=None, index_col=0, nrows=100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# REMOVE-CELL\n",
    "# References and Notes\n",
    "Refer to  https://www.netquest.com/blog/en/random-sampling-stratified-sampling for sampling techniques"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c1728eb1d2e5aa0ad9cb608f2ae480dc35c5197350e729ffcd56015e38fc7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deepcvr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
