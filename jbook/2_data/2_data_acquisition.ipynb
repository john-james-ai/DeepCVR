{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# REMOVE-CELL\n",
    "import os\n",
    "home = \"/home/john/projects/DeepCVR/\"\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(section_22_data_acquisition)=\n",
    "# Data Acquisition and Ingestion\n",
    "Wrangling, munging, cleansing and manipulating data are the irreducible factors of machine learning and its value equation. Statistical inference, predictive analytics,..., problem solving with math and machines require the right data, in the right volumes to be in a structure and format useful to the inference and predictive analytics task. This section describes the processes by which we obtain and convert our source data into forms and formats suitable for analysis and modeling. \n",
    "\n",
    "## Data Model Design Principlies\n",
    "Yet, as we kick-off reportedly the most time-consuming phase of a machine learning project, let's align on a few design principles that will inform decisions around reproducibility, transparency, productivity, and efficiency.\n",
    "\n",
    "1. **Simplicity**: This is a fairly simple data migration and the aim is to keep it so while providing a detailed account of the data preparation processes, software and toolstacks used.\n",
    "2. **Declarative Pipelines**: Pipelines, or more formally, networks of directed acyclic graphs (DAGs) will be defined in declarative YAML-based configuration files.  Whereas imperative programming approaches decompose problems into a collection of computational steps that must be carried out, declarative programming abstracts away control flow for logic required to perform an action, by instead stating the task or desired outcome without explicitly listing the commands required to complete the task. Defining pipelines in this way emphasizes 'what' must be done rather than 'how'. Achieving simplicity through restraint, declarative pipelines have a restricted, simpler syntax, allowing for less error-prone, more structured, manageable, and scalable pipeline development. Easier to read, write, and maintain, declarative pipelines are the central idea behind modern 'Pipeline as Code' approaches.\n",
    "3. **Parallel Functional Style Implementation**: For discrete tasks, operators should emphasize a stateless functional programmining paradigm that defines operations only in terms of functions or methods, their inputs, and their return values. Small functions are deterministic and composable into modules that cannot be affected by any mutable state or unintended side-effect. Defining operations in a functional paradigm supports modularity, easier debugging, testing, and verification.   \n",
    "4. **Normalize First**: Design target database schemas to third normal form (3NF), to mitigate data anomalies, ensure referential integrity, and eliminate duplication and redundancies. Denormalize into dimensions and star schemas as dimensions emerge to materially improve inference and consumability of the data.\n",
    "\n",
    "## Data Ingestion Framework\n",
    "Our development framework aligns our design principles with execution. As a general-purpose, high-level, multi-paradigm programming language, Python fully supports functional, object oriented, procedural and scripting programming paradigms for rapid development and prototyping. Python cleans up after itself. Memory for Python objects is dynamic allocated on a private heap and garbage collected by the Python Memory Manager automatically when no longer needed. For in-memory analytics, Pandas further enhances Python's memory cost performance through efficient storage of numeric and categorical data types. The Pandas DataFrame API has become somewhat of a standard, implemented and extended by an ecosystem that includes Koalas, a DataFrame interface on top of Apache Spark. In fact, we will be exploiting Spark's Pandas User-Defined Function (UDF) facility with PySpark to parallelize and scale DataFrame processing across multiple cores during the Transform stage of our ETL. Finally, our database, consisting of millions of advertising impressions and billions of user and item feature structures will be running MySQL. Version 8.0.28 includes a host of improvements designed to take full advantage of modern hardware and operating system resources and efficiencies.\n",
    "\n",
    "## Data Ingestion Module\n",
    "\n",
    "\n",
    "\n",
    " Our database engine, MySQL, has been chosen for its performance, scalability and     \n",
    "3. The imperative components of the ETL module include the DagBuilder, an ExtractDAG, TransformDAG, and a LoadDAG. DAG tasks will be performed by python Operator objects.   \n",
    "3. The datasets are reasonably large, and certain transformations must occur at the instance level. We will parallelize were feasible with Apache Spark.\n",
    "4. Our database is largely comprised of billions of feature structures. For its performance, scalability, and simplicity, MySQL will provide the backend database for analysis.\n",
    "\n",
    "## Target Data Model\n",
    "For the analysis stage, we will restructure the training data into third normal form (3NF), a relational database schema design originally defined by E.F. Codd in the early 1970's. A 3NF database minimizes data anomalies, guarantees referential integrity and ensures that every non-key attribute provide true evidence about the table key, the whole key, and nothing but the key, \"so help me Codd\" {cite}diehrDatabaseManagement1989`.  \n",
    "\n",
    "{numref}`s2t` relates the source to target database structure.\n",
    "\n",
    "```{figure} ../images/s2t.png\n",
    "---\n",
    "name: s2t\n",
    "align: center\n",
    "alt: Source to Target Model\n",
    "---\n",
    "Source to Target (S2T) Model\n",
    "```\n",
    "As depicted in {numref}`s2t`, our sample and common features datasets will become a relational database containing:  \n",
    "\n",
    "- a **sample** table holding the sample_id, the target click and conversion labels and a common_feature_index, \n",
    "- a **feature** *reference* table holding the global feature ids and their associated names,  \n",
    "- a **sample_feature** table comprised of one or more feature structures (id, value) for each sample, and \n",
    "- a **common_feature_group** table holding feature structures commmon among many samples. \n",
    "\n",
    "## Data Acquisition and Ingestion Process\n",
    "The roots of the extract transform load (ETL) design pattern extend back to the centralized data repositories of the 1970s and the data warehouses of the 1980s and early 1990s. Today, ETL and its variants describe doctrine for obtaining data from disparate systems, staging and converting the data to a suitable format and integrating the data into a data warehouse or target environment. Adopting the ETL design pattern  \n",
    "or similar storage facility.   and persisting  and is doctrine for   moving from our source to our target data model has its  The remaining subsections will be devoted to the construction and execution of a simple, configuration-based, automated and reproducible data ingestion pipeline that \n",
    "\n",
    "- **extracts** the data from its [source](s3://deepcvr-data/production/taobao_train.tar.gz), \n",
    "- **transforms** it into a usable and reliable structure, then \n",
    "- **loads** the data into a [database](https://www.mysql.com/) for downstream analysis and modeling. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The main components of this extract-transform-load (ETL) pipeline are put forward in {numref}`etl_dag`\n",
    "\n",
    "```{figure} ../images/ETL-DAG.png\n",
    "---\n",
    "name: etl_dag\n",
    "align: center\n",
    "alt: Extract Transform Load\n",
    "---\n",
    "Extract Transform Load\n",
    "```\n",
    "\n",
    "\n",
    "Allons-y!\n",
    "\n",
    "## Extract\n",
    "The extract phase is comprised of two steps. The first step is executed by the S3Downloader executable in the deepcvr.data.extract directory. It downloads the data from the production folder of the deepcvr-data AWS bucket and stores the data in the data/external directory. The second step, is performed by the Decompress class in the deepcvr.data.extract module. It unpacks the data from its GZIP archive and stores the raw data in csv format in the data/raw/ directory. The yaml configuration file 'extract.yml' file below describes a simple extract directed acyclic graph (DAG) in terms of the task executables, and their parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "dag_id: extract_dag\n",
    "dag_description: Extract phase of the ETL\n",
    "tasks:\n",
    "  1:\n",
    "    task: S3Downloader\n",
    "    module: deepcvr.data.extract\n",
    "    task_id: 1\n",
    "    task_name: s3_download\n",
    "    task_params:\n",
    "      bucket: deepcvr-data\n",
    "      folder: production/\n",
    "      destination: data/external/\n",
    "      force: False\n",
    "  2:\n",
    "    task: Decompress\n",
    "    module: deepcvr.data.extract\n",
    "    task_id: 2\n",
    "    task_name: decompress_source_files\n",
    "    task_params:\n",
    "      source: data/external/\n",
    "      destination: data/raw/\n",
    "      force: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from deepcvr.base.dag import DagBuilder\n",
    "from deepcvr.utils.config import config_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:task_event:S3Downloader.execute called with dict_values([1, 's3_download', OrderedDict([('bucket', 'deepcvr-data'), ('folder', 'production/'), ('destination', 'data/external/'), ('force', False)]), 'deepcvr-data', 'production/', 'data/external/', False, None])\n",
      "DEBUG:deepcvr.data.extract:\tStarted S3Downloader execute\n",
      "DEBUG:botocore.hooks:Changing event name from creating-client-class.iot-data to creating-client-class.iot-data-plane\n",
      "DEBUG:botocore.hooks:Changing event name from before-call.apigateway to before-call.api-gateway\n",
      "DEBUG:botocore.hooks:Changing event name from request-created.machinelearning.Predict to request-created.machine-learning.Predict\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.autoscaling.CreateLaunchConfiguration to before-parameter-build.auto-scaling.CreateLaunchConfiguration\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.route53 to before-parameter-build.route-53\n",
      "DEBUG:botocore.hooks:Changing event name from request-created.cloudsearchdomain.Search to request-created.cloudsearch-domain.Search\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.autoscaling.CreateLaunchConfiguration.complete-section to docs.*.auto-scaling.CreateLaunchConfiguration.complete-section\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.logs.CreateExportTask to before-parameter-build.cloudwatch-logs.CreateExportTask\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.logs.CreateExportTask.complete-section to docs.*.cloudwatch-logs.CreateExportTask.complete-section\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.cloudsearchdomain.Search to before-parameter-build.cloudsearch-domain.Search\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.cloudsearchdomain.Search.complete-section to docs.*.cloudsearch-domain.Search.complete-section\n",
      "DEBUG:botocore.loaders:Loading JSON file: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/boto3/data/s3/2006-03-01/resources-1.json\n",
      "DEBUG:botocore.utils:IMDS ENDPOINT: http://169.254.169.254/\n",
      "DEBUG:botocore.credentials:Looking for credentials via: env\n",
      "DEBUG:botocore.credentials:Looking for credentials via: assume-role\n",
      "DEBUG:botocore.credentials:Looking for credentials via: assume-role-with-web-identity\n",
      "DEBUG:botocore.credentials:Looking for credentials via: sso\n",
      "DEBUG:botocore.credentials:Looking for credentials via: shared-credentials-file\n",
      "DEBUG:botocore.credentials:Looking for credentials via: custom-process\n",
      "DEBUG:botocore.credentials:Looking for credentials via: config-file\n",
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n",
      "DEBUG:botocore.loaders:Loading JSON file: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/botocore/data/endpoints.json\n",
      "DEBUG:botocore.hooks:Event choose-service-name: calling handler <function handle_service_name_alias at 0x7fda3d027430>\n",
      "DEBUG:botocore.loaders:Loading JSON file: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/botocore/data/s3/2006-03-01/service-2.json\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7fda3d04e940>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function lazy_call.<locals>._handler at 0x7fda3cdaaee0>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7fda3d04e700>\n",
      "DEBUG:botocore.endpoint:Setting s3 timeout as (60, 60)\n",
      "DEBUG:botocore.loaders:Loading JSON file: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/botocore/data/_retry.json\n",
      "DEBUG:botocore.client:Registering retry handlers for service: s3\n",
      "DEBUG:boto3.resources.factory:Loading s3:s3\n",
      "DEBUG:boto3.resources.factory:Loading s3:Bucket\n",
      "DEBUG:boto3.resources.model:Renaming Bucket attribute name\n",
      "DEBUG:botocore.hooks:Event creating-resource-class.s3.Bucket: calling handler <function lazy_call.<locals>._handler at 0x7fda3cde00d0>\n",
      "DEBUG:botocore.loaders:Loading JSON file: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/botocore/data/s3/2006-03-01/paginators-1.json\n",
      "DEBUG:boto3.resources.collection:Calling paginated s3:list_objects with {'Bucket': 'deepcvr-data', 'Delimiter': '/t', 'Prefix': 'production/'}\n",
      "DEBUG:botocore.hooks:Event before-parameter-build.s3.ListObjects: calling handler <function set_list_objects_encoding_type_url at 0x7fda3cfc8dc0>\n",
      "DEBUG:botocore.hooks:Event before-parameter-build.s3.ListObjects: calling handler <function validate_bucket_name at 0x7fda3cfc2af0>\n",
      "DEBUG:botocore.hooks:Event before-parameter-build.s3.ListObjects: calling handler <bound method S3RegionRedirector.redirect_from_cache of <botocore.utils.S3RegionRedirector object at 0x7fda3c82beb0>>\n",
      "DEBUG:botocore.hooks:Event before-parameter-build.s3.ListObjects: calling handler <bound method S3ArnParamHandler.handle_arn of <botocore.utils.S3ArnParamHandler object at 0x7fda3c82bf70>>\n",
      "DEBUG:botocore.hooks:Event before-parameter-build.s3.ListObjects: calling handler <function generate_idempotent_uuid at 0x7fda3cfc2940>\n",
      "DEBUG:botocore.hooks:Event before-call.s3.ListObjects: calling handler <function add_expect_header at 0x7fda3cfc2e50>\n",
      "DEBUG:botocore.hooks:Event before-call.s3.ListObjects: calling handler <bound method S3RegionRedirector.set_request_url of <botocore.utils.S3RegionRedirector object at 0x7fda3c82beb0>>\n",
      "DEBUG:botocore.hooks:Event before-call.s3.ListObjects: calling handler <function inject_api_version_header_if_needed at 0x7fda3cfc91f0>\n",
      "DEBUG:botocore.endpoint:Making request for OperationModel(name=ListObjects) with params: {'url_path': '/deepcvr-data', 'query_string': {'delimiter': '/t', 'prefix': 'production/', 'encoding-type': 'url'}, 'method': 'GET', 'headers': {'User-Agent': 'Boto3/1.20.24 Python/3.8.12 Linux/5.10.60.1-microsoft-standard-WSL2 Botocore/1.23.24 Resource'}, 'body': b'', 'url': 'https://s3.amazonaws.com/deepcvr-data?delimiter=%2Ft&prefix=production%2F&encoding-type=url', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x7fda3cd43970>, 'has_streaming_input': False, 'auth_type': None, 'encoding_type_auto_set': True, 'signing': {'bucket': 'deepcvr-data'}}}\n",
      "DEBUG:botocore.hooks:Event request-created.s3.ListObjects: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7fda3c941be0>>\n",
      "DEBUG:botocore.hooks:Event choose-signer.s3.ListObjects: calling handler <bound method S3EndpointSetter.set_signer of <botocore.utils.S3EndpointSetter object at 0x7fda3c647040>>\n",
      "DEBUG:botocore.hooks:Event choose-signer.s3.ListObjects: calling handler <bound method ClientCreator._default_s3_presign_to_sigv2 of <botocore.client.ClientCreator object at 0x7fda3cdce220>>\n",
      "DEBUG:botocore.hooks:Event choose-signer.s3.ListObjects: calling handler <function set_operation_specific_signer at 0x7fda3cfc2820>\n",
      "DEBUG:botocore.hooks:Event before-sign.s3.ListObjects: calling handler <bound method S3EndpointSetter.set_endpoint of <botocore.utils.S3EndpointSetter object at 0x7fda3c647040>>\n",
      "DEBUG:botocore.utils:Defaulting to S3 virtual host style addressing with path style addressing fallback.\n",
      "DEBUG:botocore.utils:Checking for DNS compatible bucket for: https://s3.amazonaws.com/deepcvr-data?delimiter=%2Ft&prefix=production%2F&encoding-type=url\n",
      "DEBUG:botocore.utils:URI updated to: https://deepcvr-data.s3.amazonaws.com/?delimiter=%2Ft&prefix=production%2F&encoding-type=url\n",
      "DEBUG:botocore.auth:Calculating signature using v4 auth.\n",
      "DEBUG:botocore.auth:CanonicalRequest:\n",
      "GET\n",
      "/\n",
      "delimiter=%2Ft&encoding-type=url&prefix=production%2F\n",
      "host:deepcvr-data.s3.amazonaws.com\n",
      "x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "x-amz-date:20220325T232818Z\n",
      "\n",
      "host;x-amz-content-sha256;x-amz-date\n",
      "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "DEBUG:botocore.auth:StringToSign:\n",
      "AWS4-HMAC-SHA256\n",
      "20220325T232818Z\n",
      "20220325/us-east-1/s3/aws4_request\n",
      "c6a02205874c66a0ca6b8dfac117ed667fc05e305df0c32efbc863d52c75ded7\n",
      "DEBUG:botocore.auth:Signature:\n",
      "9d11860a5fe69a526fbfd2a98444cec479eb5616ae2ceef3b713b16152fa6725\n",
      "DEBUG:botocore.endpoint:Sending http request: <AWSPreparedRequest stream_output=False, method=GET, url=https://deepcvr-data.s3.amazonaws.com/?delimiter=%2Ft&prefix=production%2F&encoding-type=url, headers={'User-Agent': b'Boto3/1.20.24 Python/3.8.12 Linux/5.10.60.1-microsoft-standard-WSL2 Botocore/1.23.24 Resource', 'X-Amz-Date': b'20220325T232818Z', 'X-Amz-Content-SHA256': b'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'Authorization': b'AWS4-HMAC-SHA256 Credential=AKIAZQZERAVL4TAG26EO/20220325/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=9d11860a5fe69a526fbfd2a98444cec479eb5616ae2ceef3b713b16152fa6725'}>\n",
      "DEBUG:botocore.httpsession:Certificate path: /home/john/anaconda3/envs/deepcvr/lib/python3.8/site-packages/certifi/cacert.pem\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): deepcvr-data.s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://deepcvr-data.s3.amazonaws.com:443 \"GET /?delimiter=%2Ft&prefix=production%2F&encoding-type=url HTTP/1.1\" 200 None\n",
      "DEBUG:botocore.parsers:Response headers: {'x-amz-id-2': 'm/Y0lSt+fMbat/DvqG11DuW4mwj9QM8hwjLYC4QcTR7G8I/T1pK7rqVN1aMwrfcH4vPcYEfqzRQ=', 'x-amz-request-id': 'KSVWTTWGTRWCMRQJ', 'Date': 'Fri, 25 Mar 2022 23:28:21 GMT', 'x-amz-bucket-region': 'us-east-1', 'Content-Type': 'application/xml', 'Transfer-Encoding': 'chunked', 'Server': 'AmazonS3'}\n",
      "DEBUG:botocore.parsers:Response body:\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"><Name>deepcvr-data</Name><Prefix>production/</Prefix><Marker></Marker><MaxKeys>1000</MaxKeys><Delimiter>/t</Delimiter><EncodingType>url</EncodingType><IsTruncated>false</IsTruncated><Contents><Key>production/</Key><LastModified>2022-03-09T03:37:42.000Z</LastModified><ETag>&quot;d41d8cd98f00b204e9800998ecf8427e&quot;</ETag><Size>0</Size><Owner><ID>a465bca17a5d988564817408015bc4c2104e640aa0f2b769767a637c871e93d7</ID><DisplayName>john.james.ai.studio</DisplayName></Owner><StorageClass>STANDARD</StorageClass></Contents><Contents><Key>production/taobao_test.tar.gz</Key><LastModified>2022-03-09T03:38:39.000Z</LastModified><ETag>&quot;84682cbc2df92d3d22dbd4e22080f69f-300&quot;</ETag><Size>5029299086</Size><Owner><ID>a465bca17a5d988564817408015bc4c2104e640aa0f2b769767a637c871e93d7</ID><DisplayName>john.james.ai.studio</DisplayName></Owner><StorageClass>STANDARD</StorageClass></Contents><Contents><Key>production/taobao_train.tar.gz</Key><LastModified>2022-03-09T03:38:39.000Z</LastModified><ETag>&quot;9dce110393803b2848372f4876833838-263&quot;</ETag><Size>4405491733</Size><Owner><ID>a465bca17a5d988564817408015bc4c2104e640aa0f2b769767a637c871e93d7</ID><DisplayName>john.james.ai.studio</DisplayName></Owner><StorageClass>STANDARD</StorageClass></Contents></ListBucketResult>'\n",
      "DEBUG:botocore.hooks:Event needs-retry.s3.ListObjects: calling handler <botocore.retryhandler.RetryHandler object at 0x7fda3c82be50>\n",
      "DEBUG:botocore.retryhandler:No retry needed.\n",
      "DEBUG:botocore.hooks:Event needs-retry.s3.ListObjects: calling handler <bound method S3RegionRedirector.redirect_from_error of <botocore.utils.S3RegionRedirector object at 0x7fda3c82beb0>>\n",
      "DEBUG:botocore.hooks:Event after-call.s3.ListObjects: calling handler <function decode_list_object at 0x7fda3cfc8e50>\n",
      "DEBUG:boto3.resources.factory:Loading s3:ObjectSummary\n",
      "DEBUG:boto3.resources.model:Renaming ObjectSummary attribute key\n",
      "DEBUG:botocore.hooks:Event creating-resource-class.s3.ObjectSummary: calling handler <function lazy_call.<locals>._handler at 0x7fda3cde01f0>\n",
      "DEBUG:botocore.hooks:Event choose-service-name: calling handler <function handle_service_name_alias at 0x7fda3d027430>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7fda3d04e940>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function lazy_call.<locals>._handler at 0x7fda3cdaaee0>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7fda3d04e700>\n",
      "DEBUG:botocore.endpoint:Setting s3 timeout as (60, 60)\n",
      "DEBUG:botocore.client:Registering retry handlers for service: s3\n",
      "INFO:task_event:Decompress.execute called with dict_values([2, 'decompress_source_files', OrderedDict([('source', 'data/external/'), ('destination', 'data/raw/'), ('force', True)]), 'data/external/', 'data/raw/', True])\n",
      "DEBUG:deepcvr.data.extract:\tStarted Decompress execute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTask 1:\ts3 download complete.\tDuration: 0.63 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:task_event:Exception raised in execute. exception: Error -3 while decompressing data: invalid stored block lengths\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/john/projects/DeepCVR/deepcvr/utils/decorators.py\", line 74, in wrapper\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/home/john/projects/DeepCVR/deepcvr/data/extract.py\", line 158, in execute\n",
      "    tar.extractall(self._destination)\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py\", line 2028, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py\", line 2069, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py\", line 2141, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py\", line 2190, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py\", line 487, in read\n",
      "    uncompress = self._decompressor.decompress(buf, size)\n",
      "zlib.error: Error -3 while decompressing data: invalid stored block lengths\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid stored block lengths",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/john/projects/DeepCVR/jbook/2_data/2_data_acquisition.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/john/projects/DeepCVR/jbook/2_data/2_data_acquisition.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m config \u001b[39m=\u001b[39m config_dag(config_filepath)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/john/projects/DeepCVR/jbook/2_data/2_data_acquisition.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m dag \u001b[39m=\u001b[39m DagBuilder(config\u001b[39m=\u001b[39mconfig)\u001b[39m.\u001b[39mbuild()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/john/projects/DeepCVR/jbook/2_data/2_data_acquisition.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m dag\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/projects/DeepCVR/deepcvr/base/dag.py:46\u001b[0m, in \u001b[0;36mDag.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/base/dag.py?line=43'>44</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/base/dag.py?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks:\n\u001b[0;32m---> <a href='file:///home/john/projects/DeepCVR/deepcvr/base/dag.py?line=45'>46</a>\u001b[0m     data \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39;49mexecute(data\u001b[39m=\u001b[39;49mdata, context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context)\n",
      "File \u001b[0;32m~/projects/DeepCVR/deepcvr/utils/decorators.py:81\u001b[0m, in \u001b[0;36mtask_event.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=78'>79</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=79'>80</a>\u001b[0m     logger\u001b[39m.\u001b[39mexception(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException raised in \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. exception: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=80'>81</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/projects/DeepCVR/deepcvr/utils/decorators.py:74\u001b[0m, in \u001b[0;36mtask_event.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=71'>72</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=72'>73</a>\u001b[0m     start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m---> <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=73'>74</a>\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=74'>75</a>\u001b[0m     end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m     <a href='file:///home/john/projects/DeepCVR/deepcvr/utils/decorators.py?line=75'>76</a>\u001b[0m     print_result(\u001b[39mself\u001b[39m, start, end)\n",
      "File \u001b[0;32m~/projects/DeepCVR/deepcvr/data/extract.py:158\u001b[0m, in \u001b[0;36mDecompress.execute\u001b[0;34m(self, data, context)\u001b[0m\n\u001b[1;32m    <a href='file:///home/john/projects/DeepCVR/deepcvr/data/extract.py?line=155'>156</a>\u001b[0m filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_source, filename)\n\u001b[1;32m    <a href='file:///home/john/projects/DeepCVR/deepcvr/data/extract.py?line=156'>157</a>\u001b[0m tar \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mopen(filepath, \u001b[39m\"\u001b[39m\u001b[39mr:gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/john/projects/DeepCVR/deepcvr/data/extract.py?line=157'>158</a>\u001b[0m tar\u001b[39m.\u001b[39;49mextractall(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_destination)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py:2028\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2025'>2026</a>\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2026'>2027</a>\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2027'>2028</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2028'>2029</a>\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2030'>2031</a>\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2031'>2032</a>\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py:2069\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2065'>2066</a>\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2067'>2068</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2068'>2069</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2069'>2070</a>\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2070'>2071</a>\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2071'>2072</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2072'>2073</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py:2141\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2137'>2138</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, tarinfo\u001b[39m.\u001b[39mname)\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2139'>2140</a>\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[0;32m-> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2140'>2141</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2141'>2142</a>\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2142'>2143</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py:2190\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2187'>2188</a>\u001b[0m     target\u001b[39m.\u001b[39mtruncate()\n\u001b[1;32m   <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2188'>2189</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=2189'>2190</a>\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39;49msize, ReadError, bufsize)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py:247\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=244'>245</a>\u001b[0m blocks, remainder \u001b[39m=\u001b[39m \u001b[39mdivmod\u001b[39m(length, bufsize)\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=245'>246</a>\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(blocks):\n\u001b[0;32m--> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=246'>247</a>\u001b[0m     buf \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49mread(bufsize)\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=247'>248</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/tarfile.py?line=248'>249</a>\u001b[0m         \u001b[39mraise\u001b[39;00m exception(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/gzip.py:292\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=289'>290</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=290'>291</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=291'>292</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mread(size)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py?line=65'>66</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py?line=66'>67</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py?line=67'>68</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py?line=68'>69</a>\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/_compression.py?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepcvr/lib/python3.8/gzip.py:487\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=483'>484</a>\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=484'>485</a>\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=486'>487</a>\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=487'>488</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/john/anaconda3/envs/deepcvr/lib/python3.8/gzip.py?line=488'>489</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31merror\u001b[0m: Error -3 while decompressing data: invalid stored block lengths"
     ]
    }
   ],
   "source": [
    "\n",
    "config_filepath = \"config/extract.yaml\"\n",
    "config = config_dag(config_filepath)\n",
    "dag = DagBuilder(config=config).build()\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/development/staged/sample_skeleton_train.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
    "\n",
    "Our aim for the transform step, is a fully realized 3rd normal form target data model free of without redundancy, logical inconsistencies, transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
    "\n",
    "Notwithstanding, transforming our features from a series of strings to rows of feature structures will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, they are dispatched to a configurable number of CPU cores running in parallel, then the results are combined into a final single DataFrame. \n",
    "\n",
    "The source code for the transform step \n",
    "Viola! \n",
    "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
    "\n",
    "Analyzing our data\n",
    "\n",
    "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
    "\n",
    "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
    "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
    "\n",
    "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
    "\n",
    " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
    "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
    "\n",
    "![ERD](/jbook/images/ETL-DAG.png)\n",
    "\n",
    "\n",
    " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
    " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
    " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
    "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
    "\n",
    "\n",
    "## Extract\n",
    "\n",
    "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
    " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
    "\n",
    "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
    "\n",
    "- quickly illuminate structural or data quality issues \n",
    "- assess the complexity of the integration effort, and\n",
    "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
    "\n",
    "[erd](jbook/images/ERD.png)\n",
    "\n",
    "\n",
    "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
    "\n",
    "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
    "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
    "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
    "\n",
    "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
    "\n",
    "```{table} Sampling Strata\n",
    ":name: sampling_strata\n",
    "\n",
    "| Stratum | Click | Conversion | Proportion | Response                     |\n",
    "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
    "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
    "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
    "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
    "```\n",
    "A sample size \n",
    "\n",
    "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
    "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
    "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
    "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
    "\n",
    "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
    "\n",
    "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
    "\n",
    "\n",
    "\n",
    " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
    "\n",
    "### Core Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Acquisition\n",
    "Wrangling, munging, cleansing and manipulating data are irreducible variables in the machine learning and big data value equation. Statistical inference, predictive analytics, and problem solving with machines and math require data, in the right format, volume, and veracity. In this section, we design, build and execute a simple, automated and reproducible data ingestion pipeline that extracts the data from its source, transforms it into a usable and reliable resource, then loads the data into a database for downstream analysis and modeling. The main components are put forward as follows:\n",
    "\n",
    "## Extract\n",
    "Our ETL pipeline is defined using declarative pipeline syntax - basic statements and expressions which sequence the parameterized tasks that collectively execute the ETL process. First, the data are downloaded from an Amazon S3 instance, unzipped, persisted, and this raw data are registered as assets in the metadata database. Column headings are added, partitions are assigned, andd the data are stored in a staging area for the transformation step. \n",
    "\n",
    "## Transform\n",
    "Some 23 user, demographic, behavioural, and item features are split among two files: the impressions file containing a single ad impression per row, and a common features file that aggregates lists of features common among many sample impressions. As depicted in the entity relationship diagram below, the impressions file contains our targets, the click and conversion labels, a unique sample id, a feature count, and a few gigabytes of strings containing feature lists. Our common features dataset is similarly formatted. A few samples are printed for illustration purposes.\n",
    "\n",
    "Our aim for the transform step, is a fully realized 3rd normal target data model free of redundancy, and logical inconsistencies, inappropriate and transitive dependencies, and read/write anomalies. Normalization improves memory, cpu, and disk efficiency, boosts ad-hoc query processing and reduces the computational effort associated with big data analytics. Not all optimization is premature. \n",
    "\n",
    "Notwithstanding, transforming our feature data will involve computationally inefficient row-wise dataframe operations on some 88 million rows. Fortunately, Apache Spark's Pandas UDF functions implement a so-called 'split-apply-combine' pattern in which a Spark DataFrame is split into groups, a function is applied to each group, and dispatched to one of a configurable number of CPU cores, then results are combined into a final single DataFrame. \n",
    "\n",
    "The source code for the transform step \n",
    "Viola! \n",
    "dispatched to which allows one to split trials using Pandas apply method on a sample dataset were not Fortunately, Spark's recent    row-wise dataframe operations that can't task will involve 84 million costly row-wise \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " in Memory, cpu, and disk utilization of 3NFare optimized while the additiona, memory requirement, faster disk operations, Disk operations, memory utilization, query response times are advantaged by a 3NF database design and and as we move to the exploratory data analysis work,  \n",
    "\n",
    "Analyzing our data\n",
    "\n",
    "Space and time complexity oThird normal form provides flexibility, ensures referential integrity, and can be considered increases data processing efficiency, reduces storage space, , ideal for online transaction processing (OLTP)   with referential integrity,  is to parse, extract, and convert the data  the data  features into into a 3rd normal form (3NF), thereby eliminating redundancy, ensuring referential integrity, and simplify data management and exploratory analysis. \n",
    "\n",
    "Unfortunately, this parsing exercise involves a rather tedious, row-wise treatment that can't be easily vectorized. Processing 84 million rows of the data. \n",
    "Unfortunately, the structure of the feature data will require row-wise parsing - a rather computationally burdensome task \n",
    "\n",
    "This row-wise parsing exercise can't be efficiently vectorized, but park  \n",
    "\n",
    " and management.   form  the features The  Each impression in the impression file contains a list of one ore more feature structures concatenated into strings, which delimited by selected non-printable ASCII characters. Similarly, lists of feature structures \n",
    "Our feature set includes some 23 user demographic, behavioural, transactional and item features concatenated, and compressed into two strings stored across the two files which collectively make up our training set.  files which collectively series of strings across across two files. impressions file contains:\n",
    "\n",
    "![ERD](/jbook/images/ETL-DAG.png)\n",
    "\n",
    "\n",
    " the the target click and conversion labels, a feature count, a sample id and a series of strings containing one ore more feature lists. The second file, contains a similar collection of features lists organized into a series of concatenated feature structures.\n",
    " features that are common among many of the samples in the impressions file.   common feature file contains a collection of feature groups that have been aggregated , packed into ASCII character delimited strings containing the feature structures. Each structure contains and id, a feature name and a corresponding feature value. The primary aim of the transform step is to parse the features structures into the individual features and samples. Concretely, our core impressions will be split into an impressions table, containing a single observation for each  impression, and a features table with one-to-many foreign references to the impressions   into file will be transformed into features these features into feature structures that can be analyzed and processed. The sample below  containing   in comma separated strings.concatenated and encoded into comma separated strings  strings   \n",
    " and partiti in the metatadatabase  that  the tasks to be completed, the parameters  \n",
    "Step 1. Download our data from its Amazon S3 instance, unzip the compressed archives, persist and register the raw data. Next, column names are added, partitions are assigned, and the assets are registered in the metadata database before staging the data for the transformation phase.  \n",
    "\n",
    "\n",
    "## Extract\n",
    "\n",
    "The remote S3 datasource is downloaded, decompressed, and stored in the raw data directory. A staging process adds column names and assigns each observation a partition number to support parallel processing in the transform stage.\n",
    " partitions   this data management framework is to download the source data into the external data directory on the local drive. It is then decompressed from its GZIP archive and migrated to teh loca\n",
    "\n",
    "We begin the ETL design with a quick assessment of the data vis-a-vis our (heretofore unspecified) target database in order to:\n",
    "\n",
    "- quickly illuminate structural or data quality issues \n",
    "- assess the complexity of the integration effort, and\n",
    "- evaluate the utility of the dataset and its attributes to the analysis and modeling efforts. \n",
    "\n",
    "[erd](jbook/images/ERD.png)\n",
    "\n",
    "\n",
    "To reduce our computational burden, advance the ETL analysis, design, and development effort, a multivariate multi-objective stratified optimal distribution-preserving class-proportional downsampling dataset will be created that reflects the structure of the entire training set.\n",
    "\n",
    "sampling and allocation data profiling effort and the analysis, design, and ToTo mitigate computational burden  and of Analyzing and manipulating 90 million observations across 40 Gb To reduce computational cost and to facilitate the data profiling and discovery effort, a random sample   ETL development  deTo address the class imbalance question, data generation and sampling techniques have evolved    \n",
    "To moderate the computational cost of analyzing and manipulating our data,  Though our dataset would not be considered big data in any modern context, the computational cost of analyzing and manipulating such datasets motivates   increases controlling the computational cost of the data acquisition and exploratory analysis efforts  motivated questions about the optimal size and allocation of data samples    analyzing and manipulating datasets of these sizes came with a computational burden \n",
    "To reduce the computational burden, multivariate proportional stratified downsampling was conducted to produce a sample dataset that reflected the distributions, diversity, and statistical properties of the full training. \n",
    "\n",
    "{numref}`sampling_strata`: Alibaba Click and Conversion Prediction (Ali-CCP) Dataset Sampling Strata\n",
    "\n",
    "```{table} Sampling Strata\n",
    ":name: sampling_strata\n",
    "\n",
    "| Stratum | Click | Conversion | Proportion | Response                     |\n",
    "|:-------:|:-----:|:----------:|:----------:|------------------------------|\n",
    "|    1    |   0   |      0     |   96.11%   | No response to ad            |\n",
    "|    2    |   1   |      0     |    3.89%   | Click through                |\n",
    "|    3    |   1   |      1     |    0.02%   | Click-through and conversion |\n",
    "```\n",
    "A sample size \n",
    "\n",
    "Next, an optimal total sample size was calculated and stratified random sampling from each strata was conducted in accordance with the distribution conducted to preserve \n",
    "   was  , Analyzing and manipulating mid-sized datasets To mitigate some computational cost \n",
    "Combined, we have approximately 86 million observations split almost evenly between the training and test sets. Restricting our   observations in our training and test sets. \n",
    "For computational convenience, we'll extract a *representative* sample from the *training* set for this stage of the analysis. And since the common features dataset extends the impression dataset, we'll treat both as a single training set of 42.3 million observations. \n",
    "\n",
    "Thus, we need to know how large a representative sample needs to be, assuming a margin of error of +/-5%. Restating the problem, we seek a dataset in which the 100(1-$\\alpha$)% confidence interval for the sample conversion rate contains the true population conversion rate with probability of at least 1-$\\alpha$. Hence, we have a 95% confidence that the true conversion rate is contained inside the 95% confidence interval. \n",
    "\n",
    "Conversions are discrete events following a binomial distribution. If $P$ is our \n",
    "\n",
    "\n",
    "\n",
    " Since   Defining *representative* in terms of conversion rate, we seek a sample size in which the sample mean conversion rate and its variance approximates the associated mean and variance of the *population* within some margin of error, say, 0.05%. Fortunately, the central limit theorem provides a principled method for     of the  and the  and  Our impressions dataset has a population of 42 million observations   Representatve Fortunately, the central limit theorem (CLT) allows us to \n",
    "\n",
    "### Core Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions = \"data/archive/production/raw/sample_skeleton_train.csv\"\n",
    "df = pd.read_csv(impressions, header=None, index_col=None)\n",
    "df.loc[(df[1]==0) & (df[2]==0)].shape[0] / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "Downloading the data from our S3 instance will take approximately 15 minutes on a standard 40 Mbps internet line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s S3Downloader deepcvr/data/download.py\n",
    "class S3Downloader:\n",
    "    \"\"\"Download operator for Amazon S3 Resources\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket\n",
    "        destination (str): Director to which all resources are to be downloaded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket: str, destination: str, force: bool = False) -> None:\n",
    "        self._bucket = bucket\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "        config = S3Config()\n",
    "        self._s3 = boto3.client(\n",
    "            \"s3\", aws_access_key_id=config.key, aws_secret_access_key=config.secret\n",
    "        )\n",
    "        self._progressbar = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "\n",
    "        object_keys = self._list_bucket_contents()\n",
    "\n",
    "        for object_key in object_keys:\n",
    "            destination = os.path.join(self._destination, object_key)\n",
    "            if not os.path.exists(destination) or self._force:\n",
    "                self._download(object_key, destination)\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Bucket resource {} already exists and was not downloaded.\".format(destination)\n",
    "                )\n",
    "\n",
    "    def _list_bucket_contents(self) -> list:\n",
    "        \"\"\"Returns a list of objects in the designated bucket\"\"\"\n",
    "        objects = []\n",
    "        s3 = boto3.resource(\"s3\")\n",
    "        bucket = s3.Bucket(self._bucket)\n",
    "        for object in bucket.objects.all():\n",
    "            objects.append(object.key)\n",
    "        return objects\n",
    "\n",
    "    def _download(self, object_key: str, destination: str) -> None:\n",
    "        \"\"\"Downloads object designated by the object ke if not exists or force is True\"\"\"\n",
    "\n",
    "        response = self._s3.head_object(Bucket=self._bucket, Key=object_key)\n",
    "        size = response[\"ContentLength\"]\n",
    "\n",
    "        self._progressbar = progressbar.progressbar.ProgressBar(maxval=size)\n",
    "        self._progressbar.start()\n",
    "\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        try:\n",
    "            self._s3.download_file(\n",
    "                self._bucket, object_key, destination, Callback=self._download_callback\n",
    "            )\n",
    "            logger.info(\"Download of {} Complete!\".format(object_key))\n",
    "        except NoCredentialsError:\n",
    "            msg = \"Credentials not available for {} bucket\".format(self._bucket)\n",
    "            raise NoCredentialsError(msg)\n",
    "\n",
    "    def _download_callback(self, size):\n",
    "        self._progressbar.update(self._progressbar.currval + size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = S3Downloader(bucket=S3_BUCKET, destination=DIRECTORY_EXTERNAL)\n",
    "downloader.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Raw Data\n",
    "Here, we extract the compressed files into a raw data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s Extractor deepcvr/data/extract.py\n",
    "class Extractor:\n",
    "    \"\"\"Decompresses a gzip archive, stores the raw data\n",
    "\n",
    "    Args:\n",
    "        source (str): The filepath to the source file to be decompressed\n",
    "        destination (str): The destination directory into which data shall be stored.\n",
    "        filetype (str): The file extension for the uncompressed data\n",
    "        force (bool): Forces extraction even when files already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source: str, destination: str, force: bool = False) -> None:\n",
    "\n",
    "        self._source = source\n",
    "        self._destination = destination\n",
    "        self._force = force\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        \"\"\"Extracts and stores the data, then pushes filepaths to xCom.\"\"\"\n",
    "        logger.debug(\"\\tSource: {}\\tDestination: {}\".format(self._source, self._destination))\n",
    "\n",
    "        # If all 4 raw files exist, it is assumed that the data have been downloaded\n",
    "        n_files = len(os.listdir(self._destination))\n",
    "        if n_files < 4:\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as tempdir:\n",
    "                # Recursively extract data and store in destination directory\n",
    "                self._extract(source=self._source, destination=tempdir)\n",
    "\n",
    "    def _extract(self, source: str, destination: str) -> None:\n",
    "        \"\"\"Extracts the data and returns the extracted filepaths\"\"\"\n",
    "\n",
    "        logger.debug(\"\\t\\tOpening {}\".format(source))\n",
    "        data = tarfile.open(source)\n",
    "\n",
    "        for member in data.getmembers():\n",
    "            if self._is_csvfile(filename=member.name):\n",
    "                if self._not_exists_or_force(member_name=member.name):\n",
    "                    logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, self._destination))\n",
    "                    data.extract(member, self._destination)  # Extract to destination\n",
    "                else:\n",
    "                    pass  # Do nothing if the csv file already exists and Force is False\n",
    "\n",
    "            else:\n",
    "                logger.debug(\"\\t\\tExtracting {} to {}\".format(member.name, destination))\n",
    "                data.extract(member, destination)  # Extract to tempdirectory\n",
    "\n",
    "    def _not_exists_or_force(self, member_name: str) -> bool:\n",
    "        \"\"\"Returns true if the file doesn't exist or force is True.\"\"\"\n",
    "        filepath = os.path.join(self._destination, member_name)\n",
    "        return not os.path.exists(filepath) or self._force\n",
    "\n",
    "    def _is_csvfile(self, filename: str) -> bool:\n",
    "        \"\"\"Returns True if filename is a csv file, returns False otherwise.\"\"\"\n",
    "        return \".csv\" in filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor(source=FILEPATH_EXTERNAL_TRAIN, destination=DIRECTORY_RAW)\n",
    "filenames = extractor.execute()\n",
    "os.listdir(DIRECTORY_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Dataset Preprocessing\n",
    "Let's take a preliminary look at the core training dataset.\n",
    "### Core Raw Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TEST_CORE, header=None, index_col=[0], nrows=10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH_RAW_TRAIN_COMMON, header=None, index_col=0, nrows=100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: \n",
    "\n",
    "| Column | Field                                  |\n",
    "|--------|----------------------------------------|\n",
    "| 0      | Sample-id                              |\n",
    "| 1      | Click Label                            |\n",
    "| 2      | Conversion Label                       |\n",
    "| 3      | Common Features Foreign Key            |\n",
    "| 4      | Number of features in the feature list |\n",
    "| 5      | Feature List                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# REMOVE-CELL\n",
    "# References and Notes\n",
    "Refer to  https://www.netquest.com/blog/en/random-sampling-stratified-sampling for sampling techniques"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c1728eb1d2e5aa0ad9cb608f2ae480dc35c5197350e729ffcd56015e38fc7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deepcvr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
