{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(section_22_data_acquisition)=\n",
    "# Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these datasets we will implement the extract load transform (ELT) pattern, a variant of the well established extract transform load pattern from the 70's. Rather than transform the data prior to loading, we'll load the raw data into the database and perform transformations there. Concretely, our data pipeline will:\n",
    "\n",
    "1. Extract: Download the data from its source site to our raw data directory \n",
    "2. Load: Create a MySQL database and load the raw data \n",
    "3. Transform: Not sure what this will entail as of yet.\n",
    "\n",
    "Once the data are loaded, we'll extract a sample representating the distribution of labels in the full training dataset. The data pipeline will be operationalized so that the transformations applied to the training set can be applied to the test set without data leakage.\n",
    "\n",
    "```{tip} Reproducibility\n",
    ":class: dropdown\n",
    "To run this notebook on the cloud, click on the shuttle icon in the external links at the top right of the page. This will launch the notebook on a Google Colab instance. The data may be obtained from [Alibaba's Tianchi website](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408); however, we will be downloading the files from an Amazon AWS server. \n",
    "```\n",
    "```{tip} Acquisition Imports\n",
    ":class: dropdown \n",
    "To view source code in this notebook, click on the icon 'Click to show' to the right.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "from IPython import display\n",
    "import logging\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import tarfile\n",
    "import shutil\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# REMOVE-CELL\n",
    "home = \"/home/john/projects/deepcvr\"\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "### Download Data\n",
    "Here we download and extract the data from compressed archive files. The Amazon AWS access credentials are stored in a configuration file which will parameterize our download function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Raw Data\n",
    "The files have been downloaded to our project external data directory at data/external. Next, we'll extract the CSV files from the archive and secure the raw data in our raw directory, data/raw. The following code extracts the data and returns the filepaths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s Extractor deepcvr/data/extract.py\n",
    "class Extractor:\n",
    "    \"\"\"Decompresses a gzip archive, stores the raw data and pushes the filepaths to xCom\n",
    "\n",
    "    Args:\n",
    "        source (str): The source filepath\n",
    "        destination (str): The destination directory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self._source = None\n",
    "        self._destination = None\n",
    "\n",
    "    def execute(self, source: str, destination: str) -> None:\n",
    "        \"\"\"Extracts and stores the data, then pushes filepaths to xCom. \"\"\"\n",
    "\n",
    "        self._source = source\n",
    "        self._destination = destination\n",
    "\n",
    "        if not self._exists():\n",
    "            # Recursively extract data and store in destination directory\n",
    "            self._extract(self._source)\n",
    "\n",
    "        # Extract filepaths for all data downloaded and extracted\n",
    "        filepaths = self._get_filepaths()\n",
    "\n",
    "        return filepaths\n",
    "\n",
    "    def _exists(self) -> bool:\n",
    "        \"\"\"Checks destination directory and returns True if not empty. False otherwise.\"\"\"\n",
    "\n",
    "        return len(os.listdir(self._destination)) > 0\n",
    "\n",
    "    def _extract(self, filepath: str) -> None:\n",
    "        \"\"\"Extracts the data and returns the extracted filepaths\"\"\"\n",
    "\n",
    "        if tarfile.is_tarfile(filepath):\n",
    "            with tempfile.TemporaryDirectory() as tempdirname:\n",
    "                data = tarfile.open(filepath)\n",
    "                for member in data.getmembers():\n",
    "                    # If the file already exists, skip this step\n",
    "                    filepath = os.path.join(tempdirname, member.name)\n",
    "                    data.extract(member, tempdirname)\n",
    "                    return self._extract(filepath)\n",
    "        else:\n",
    "            self._savefile(filepath)\n",
    "\n",
    "    def _savefile(self, filepath: str) -> None:\n",
    "        \"\"\"Saves file to destination and adds name and filepath to filepaths dictionary\n",
    "\n",
    "        Args:\n",
    "            filepath (str): the path to the extracted file in the temp directory\n",
    "        \"\"\"\n",
    "\n",
    "        # Create destination filepath and move the file\n",
    "        destination = os.path.join(self._destination, os.path.basename(filepath))\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        shutil.move(filepath, destination)\n",
    "\n",
    "    def _get_filepaths(self) -> dict:\n",
    "        \"\"\"Creates a dictionary of destination file paths.\"\"\"\n",
    "        filepaths = {}\n",
    "\n",
    "        filenames = os.listdir(self._destination)\n",
    "        if len(filenames) > 0:\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(self._destination, filename)\n",
    "                name = os.path.splitext(filename)[0]\n",
    "                filepaths[name] = filepath\n",
    "        else:\n",
    "            msg = \"Destination directory is empty\"\n",
    "            raise FileNotFoundError(msg)\n",
    "\n",
    "        return filepaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_skeleton_train.csv', 'common_features_train.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"data/external/taobao_train.tar.gz\"\n",
    "destination = 'data/raw'\n",
    "extractor = Extractor()\n",
    "filenames = extractor.execute(source=source, destination=destination)\n",
    "os.listdir(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bacff91692951881</td>\n",
       "      <td>9</td>\n",
       "      <td>210\u00029052218\u00031.0\u0001210\u00029064553\u00031.0\u0001210\u00029093445\u00031....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bacff91692951881</td>\n",
       "      <td>10</td>\n",
       "      <td>210\u00029109732\u00031.0\u0001210\u00029046284\u00031.0\u0001210\u00029099035\u00031....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>bacff91692951881</td>\n",
       "      <td>20</td>\n",
       "      <td>210\u00029089731\u00031.0\u0001210\u00029047560\u00031.0\u0001509\u00029511769\u00032....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bacff91692951881</td>\n",
       "      <td>13</td>\n",
       "      <td>301\u00029351665\u00031.0\u0001210\u00029050364\u00031.0\u0001210\u00029083388\u00031....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bacff91692951881</td>\n",
       "      <td>9</td>\n",
       "      <td>205\u00024945663\u00031.0\u0001301\u00029351665\u00031.0\u0001216\u00029172179\u00031....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2                 3   4  \\\n",
       "0                               \n",
       "1  0  0  bacff91692951881   9   \n",
       "2  0  0  bacff91692951881  10   \n",
       "3  1  0  bacff91692951881  20   \n",
       "4  0  0  bacff91692951881  13   \n",
       "5  0  0  bacff91692951881   9   \n",
       "\n",
       "                                                   5  \n",
       "0                                                     \n",
       "1  210\u00029052218\u00031.0\u0001210\u00029064553\u00031.0\u0001210\u00029093445\u00031....  \n",
       "2  210\u00029109732\u00031.0\u0001210\u00029046284\u00031.0\u0001210\u00029099035\u00031....  \n",
       "3  210\u00029089731\u00031.0\u0001210\u00029047560\u00031.0\u0001509\u00029511769\u00032....  \n",
       "4  301\u00029351665\u00031.0\u0001210\u00029050364\u00031.0\u0001210\u00029083388\u00031....  \n",
       "5  205\u00024945663\u00031.0\u0001301\u00029351665\u00031.0\u0001216\u00029172179\u00031....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_train_raw_filepath = os.path.join(destination, os.listdir(destination)[0])\n",
    "common_features_train_raw_filepath = os.path.join(destination, os.listdir(destination)[1])\n",
    "core_train_raw_sample = pd.read_csv(core_train_raw_filepath,nrows=1000,header=None, index_col=0)\n",
    "core_train_raw_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# REMOVE-CELL\n",
    "# References and Notes\n",
    "Refer to  https://www.netquest.com/blog/en/random-sampling-stratified-sampling for sampling techniques"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c1728eb1d2e5aa0ad9cb608f2ae480dc35c5197350e729ffcd56015e38fc7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deepcvr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
